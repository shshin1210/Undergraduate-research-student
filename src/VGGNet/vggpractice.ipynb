{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vggpractice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnfOU0IDe0Aw"
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.get_device_name(0)\n",
        "print(torch.__version__)\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cH2yC9Qe_YD"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.maual_seed(123)\n",
        "\n",
        "if devive == 'cuda':\n",
        "  torch.cuda.manual_seed_all(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUBQSYbFfLHm"
      },
      "source": [
        "''' VGG 11,113,16,19 in Pytorch '''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "  #VGG 클래스 불러올시, vgg_name 안에 입력하기 -> 바로 make_layers(cfg[vgg_name]) 함수 사용\n",
        "  def __init__(self, vgg_name):\n",
        "    super(VGG, self).__init__()\n",
        "    self.features = self._make_layers(cfg[vgg_name])\n",
        "\n",
        "    #마지막에 FC 층을 의미\n",
        "    self.classifier = nn.Linear(512,10)\n",
        "\n",
        "    #forward propagation\n",
        "    def forward(self, x):\n",
        "      out = self.features(x) # out에 features(x)를 받는다\n",
        "      out = out.view(out.size(0), -1)\n",
        "      out = self.classifier(out) #classifier 마지막 FC 층 의미(?)\n",
        "\n",
        "    #층 만들기\n",
        "    def _make_layers(self, cfg):\n",
        "      layers =[] #빈 층 리스트\n",
        "      in_channels = 3\n",
        "\n",
        "\n",
        "      #cfg dictionary 를 기반으로\n",
        "      for x in cfg:\n",
        "\n",
        "        #M cfg 안에 있다면, Maxpooling 층을 층 리스트에 넣음\n",
        "        if x == 'M':\n",
        "          layers += [nn.MaxPool2d(kernel_size =2, stride =2)]\n",
        "\n",
        "        #M 외의 것은 convolutional 층 / 숫자는 BatchNorm2d에 입력 (?)\n",
        "        else:\n",
        "          layers += [nn.Conv2d(in_chnnels, x, kernel_size =3, padding=1),\n",
        "                     nn.BatchNorm2d(x),\n",
        "                     nn.ReLU(inplace = True)\n",
        "                     ]\n",
        "          in_channels = x\n",
        "\n",
        "      # 마지막으로 AvgPool 층 추가\n",
        "      layers += [nn.AvgPool2d(kernel_size =1, stride =1)]\n",
        "\n",
        "      # Sequenetial 은 신경망 구축하는 함수임!\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "def test():\n",
        "  net = VGG('VGG16')\n",
        "  x = torch.randn(2,3,32,32)\n",
        "  y = net(x)\n",
        "  print(y.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSNqNh4UhMIX"
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "body = {\"terminal\": \"1,16011301,,\" , \"rows\": 1,\"visMode\": 1}\n",
        "r = requests.post('http://st.atb.no/New/minskjerm/DataHandler.ashx?type=departureTimes&lang=no', data = body)\n",
        "soup = bs(r.content, 'lxml')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "\n",
        "print('==> Preparing Data')\n",
        "\n",
        "#transforms.Compose는 데이터 전처리 과정을 의미한다.\n",
        "#train 데이터는 Crop 과 Flip을 통해 훈련할 때 Overfitting을 막아주고, 적은 데이터 단점을 보완\n",
        "#Normalize는 train set 데이터들의 mean과 std를 입력\n",
        "transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding =4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./cifar10', train=True, download=True, transform=transform_train\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size = 128, shuffle = True, num_workers =2\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./cifar10', train=False, download=True, transform=transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfaJt-rCiyjo"
      },
      "source": [
        "# Model\n",
        "\n",
        "print('==> Building model...')\n",
        "\n",
        "net = VGG('VGG16')\n",
        "\n",
        "net = net.to(decive)\n",
        "\n",
        "if device == 'cuda':\n",
        "  net = torch.nn.DataParallel(net)\n",
        "  cudnn.benchmark = True\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}