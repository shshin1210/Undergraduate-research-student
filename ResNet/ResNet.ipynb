{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5599ab0062894e14b3a58c0ced418629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b853cace82e9443f8bef773f7e5c94f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a401c4b91c66490bb5fb164fd5b841c8",
              "IPY_MODEL_e2207663fd5344edbe251c3bfd65b43f",
              "IPY_MODEL_b9f046d0624e4508aaa47994b7b1387c"
            ]
          }
        },
        "b853cace82e9443f8bef773f7e5c94f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a401c4b91c66490bb5fb164fd5b841c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab1ada35375e4f1fa8362d1814d1e65b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da765b1a61ff4733bc5f44be97a021ee"
          }
        },
        "e2207663fd5344edbe251c3bfd65b43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d674c8a43c094417a8698c2e4d90185a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bd0313334b74fec9eec828acd975553"
          }
        },
        "b9f046d0624e4508aaa47994b7b1387c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9acc92d059484226868fdcdb083e13dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 30680150.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_966b9032f6f34db4a87ea4bc1945f68e"
          }
        },
        "ab1ada35375e4f1fa8362d1814d1e65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da765b1a61ff4733bc5f44be97a021ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d674c8a43c094417a8698c2e4d90185a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bd0313334b74fec9eec828acd975553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9acc92d059484226868fdcdb083e13dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "966b9032f6f34db4a87ea4bc1945f68e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT1Go6ZdX2ub",
        "outputId": "f3d2bcec-b1ea-4187-e990-48363f329284"
      },
      "source": [
        "import torch \n",
        "\n",
        "torch.cuda.get_device_name(0)\n",
        "print(torch.__version__)\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu102\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLXJ_A_hX-Sf"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "\n",
        "torch.manual_seed(123) \n",
        "if device =='cuda': \n",
        "    torch.cuda.manual_seed_all(123)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Mq74Q-YicW"
      },
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        \n",
        "        #stride가 1이아니면\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "    #layer가 50이상일때\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        \n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "5599ab0062894e14b3a58c0ced418629",
            "b853cace82e9443f8bef773f7e5c94f3",
            "a401c4b91c66490bb5fb164fd5b841c8",
            "e2207663fd5344edbe251c3bfd65b43f",
            "b9f046d0624e4508aaa47994b7b1387c",
            "ab1ada35375e4f1fa8362d1814d1e65b",
            "da765b1a61ff4733bc5f44be97a021ee",
            "d674c8a43c094417a8698c2e4d90185a",
            "9bd0313334b74fec9eec828acd975553",
            "9acc92d059484226868fdcdb083e13dc",
            "966b9032f6f34db4a87ea4bc1945f68e"
          ]
        },
        "id": "vVt10XAWAWyJ",
        "outputId": "8d077049-04fe-4c64-994b-3db155931700"
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true',\n",
        "#                     help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5599ab0062894e14b3a58c0ced418629",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO3gomdCAW7V",
        "outputId": "54df7cea-3580-49f1-d6d0-f332326a5aa8"
      },
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "\n",
        "net = ResNet50()\n",
        "\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "# net = EfficientNetB0()\n",
        "# net = RegNetX_200MF()\n",
        "\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "#     # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.494 | Acc: 10.156% (13/128)\n",
            "1 391 Loss: 10.993 | Acc: 10.547% (27/256)\n",
            "2 391 Loss: 12.784 | Acc: 10.938% (42/384)\n",
            "3 391 Loss: 12.160 | Acc: 11.328% (58/512)\n",
            "4 391 Loss: 15.362 | Acc: 11.562% (74/640)\n",
            "5 391 Loss: 20.563 | Acc: 10.807% (83/768)\n",
            "6 391 Loss: 21.134 | Acc: 10.826% (97/896)\n",
            "7 391 Loss: 20.514 | Acc: 10.645% (109/1024)\n",
            "8 391 Loss: 19.647 | Acc: 10.938% (126/1152)\n",
            "9 391 Loss: 18.906 | Acc: 10.859% (139/1280)\n",
            "10 391 Loss: 17.762 | Acc: 10.511% (148/1408)\n",
            "11 391 Loss: 16.596 | Acc: 10.872% (167/1536)\n",
            "12 391 Loss: 15.614 | Acc: 10.637% (177/1664)\n",
            "13 391 Loss: 15.095 | Acc: 10.714% (192/1792)\n",
            "14 391 Loss: 14.270 | Acc: 10.521% (202/1920)\n",
            "15 391 Loss: 13.658 | Acc: 10.889% (223/2048)\n",
            "16 391 Loss: 14.001 | Acc: 10.938% (238/2176)\n",
            "17 391 Loss: 13.575 | Acc: 10.807% (249/2304)\n",
            "18 391 Loss: 12.992 | Acc: 10.650% (259/2432)\n",
            "19 391 Loss: 12.467 | Acc: 10.859% (278/2560)\n",
            "20 391 Loss: 12.086 | Acc: 10.900% (293/2688)\n",
            "21 391 Loss: 11.646 | Acc: 10.902% (307/2816)\n",
            "22 391 Loss: 11.280 | Acc: 11.175% (329/2944)\n",
            "23 391 Loss: 10.923 | Acc: 11.296% (347/3072)\n",
            "24 391 Loss: 10.587 | Acc: 11.219% (359/3200)\n",
            "25 391 Loss: 10.371 | Acc: 11.328% (377/3328)\n",
            "26 391 Loss: 10.159 | Acc: 11.256% (389/3456)\n",
            "27 391 Loss: 9.905 | Acc: 11.021% (395/3584)\n",
            "28 391 Loss: 9.657 | Acc: 10.964% (407/3712)\n",
            "29 391 Loss: 9.427 | Acc: 10.833% (416/3840)\n",
            "30 391 Loss: 9.248 | Acc: 10.761% (427/3968)\n",
            "31 391 Loss: 9.116 | Acc: 10.693% (438/4096)\n",
            "32 391 Loss: 9.024 | Acc: 10.677% (451/4224)\n",
            "33 391 Loss: 8.870 | Acc: 10.639% (463/4352)\n",
            "34 391 Loss: 8.746 | Acc: 10.603% (475/4480)\n",
            "35 391 Loss: 8.602 | Acc: 10.547% (486/4608)\n",
            "36 391 Loss: 8.502 | Acc: 10.473% (496/4736)\n",
            "37 391 Loss: 8.425 | Acc: 10.403% (506/4864)\n",
            "38 391 Loss: 8.272 | Acc: 10.417% (520/4992)\n",
            "39 391 Loss: 8.147 | Acc: 10.430% (534/5120)\n",
            "40 391 Loss: 8.020 | Acc: 10.537% (553/5248)\n",
            "41 391 Loss: 7.882 | Acc: 10.658% (573/5376)\n",
            "42 391 Loss: 7.762 | Acc: 10.683% (588/5504)\n",
            "43 391 Loss: 7.647 | Acc: 10.600% (597/5632)\n",
            "44 391 Loss: 7.552 | Acc: 10.590% (610/5760)\n",
            "45 391 Loss: 7.439 | Acc: 10.632% (626/5888)\n",
            "46 391 Loss: 7.333 | Acc: 10.622% (639/6016)\n",
            "47 391 Loss: 7.227 | Acc: 10.710% (658/6144)\n",
            "48 391 Loss: 7.139 | Acc: 10.682% (670/6272)\n",
            "49 391 Loss: 7.045 | Acc: 10.734% (687/6400)\n",
            "50 391 Loss: 7.017 | Acc: 10.662% (696/6528)\n",
            "51 391 Loss: 6.925 | Acc: 10.787% (718/6656)\n",
            "52 391 Loss: 6.840 | Acc: 10.790% (732/6784)\n",
            "53 391 Loss: 6.755 | Acc: 10.822% (748/6912)\n",
            "54 391 Loss: 6.692 | Acc: 10.810% (761/7040)\n",
            "55 391 Loss: 6.614 | Acc: 10.812% (775/7168)\n",
            "56 391 Loss: 6.560 | Acc: 10.855% (792/7296)\n",
            "57 391 Loss: 6.668 | Acc: 10.843% (805/7424)\n",
            "58 391 Loss: 6.594 | Acc: 10.911% (824/7552)\n",
            "59 391 Loss: 6.522 | Acc: 10.951% (841/7680)\n",
            "60 391 Loss: 6.461 | Acc: 10.963% (856/7808)\n",
            "61 391 Loss: 6.408 | Acc: 10.912% (866/7936)\n",
            "62 391 Loss: 6.345 | Acc: 10.875% (877/8064)\n",
            "63 391 Loss: 6.354 | Acc: 10.803% (885/8192)\n",
            "64 391 Loss: 6.299 | Acc: 10.769% (896/8320)\n",
            "65 391 Loss: 6.240 | Acc: 10.713% (905/8448)\n",
            "66 391 Loss: 6.183 | Acc: 10.693% (917/8576)\n",
            "67 391 Loss: 6.125 | Acc: 10.673% (929/8704)\n",
            "68 391 Loss: 6.076 | Acc: 10.643% (940/8832)\n",
            "69 391 Loss: 6.032 | Acc: 10.625% (952/8960)\n",
            "70 391 Loss: 5.979 | Acc: 10.618% (965/9088)\n",
            "71 391 Loss: 5.929 | Acc: 10.579% (975/9216)\n",
            "72 391 Loss: 5.880 | Acc: 10.542% (985/9344)\n",
            "73 391 Loss: 5.837 | Acc: 10.547% (999/9472)\n",
            "74 391 Loss: 5.790 | Acc: 10.594% (1017/9600)\n",
            "75 391 Loss: 5.745 | Acc: 10.639% (1035/9728)\n",
            "76 391 Loss: 5.704 | Acc: 10.643% (1049/9856)\n",
            "77 391 Loss: 5.714 | Acc: 10.587% (1057/9984)\n",
            "78 391 Loss: 5.671 | Acc: 10.572% (1069/10112)\n",
            "79 391 Loss: 5.630 | Acc: 10.615% (1087/10240)\n",
            "80 391 Loss: 5.594 | Acc: 10.571% (1096/10368)\n",
            "81 391 Loss: 5.556 | Acc: 10.623% (1115/10496)\n",
            "82 391 Loss: 5.520 | Acc: 10.599% (1126/10624)\n",
            "83 391 Loss: 5.484 | Acc: 10.575% (1137/10752)\n",
            "84 391 Loss: 5.446 | Acc: 10.616% (1155/10880)\n",
            "85 391 Loss: 5.410 | Acc: 10.620% (1169/11008)\n",
            "86 391 Loss: 5.432 | Acc: 10.623% (1183/11136)\n",
            "87 391 Loss: 5.397 | Acc: 10.627% (1197/11264)\n",
            "88 391 Loss: 5.363 | Acc: 10.648% (1213/11392)\n",
            "89 391 Loss: 5.330 | Acc: 10.668% (1229/11520)\n",
            "90 391 Loss: 5.297 | Acc: 10.611% (1236/11648)\n",
            "91 391 Loss: 5.268 | Acc: 10.615% (1250/11776)\n",
            "92 391 Loss: 5.237 | Acc: 10.585% (1260/11904)\n",
            "93 391 Loss: 5.214 | Acc: 10.580% (1273/12032)\n",
            "94 391 Loss: 5.184 | Acc: 10.559% (1284/12160)\n",
            "95 391 Loss: 5.154 | Acc: 10.531% (1294/12288)\n",
            "96 391 Loss: 5.125 | Acc: 10.486% (1302/12416)\n",
            "97 391 Loss: 5.096 | Acc: 10.491% (1316/12544)\n",
            "98 391 Loss: 5.069 | Acc: 10.472% (1327/12672)\n",
            "99 391 Loss: 5.043 | Acc: 10.523% (1347/12800)\n",
            "100 391 Loss: 5.016 | Acc: 10.589% (1369/12928)\n",
            "101 391 Loss: 4.990 | Acc: 10.578% (1381/13056)\n",
            "102 391 Loss: 4.964 | Acc: 10.566% (1393/13184)\n",
            "103 391 Loss: 4.938 | Acc: 10.577% (1408/13312)\n",
            "104 391 Loss: 4.913 | Acc: 10.573% (1421/13440)\n",
            "105 391 Loss: 4.889 | Acc: 10.540% (1430/13568)\n",
            "106 391 Loss: 4.865 | Acc: 10.521% (1441/13696)\n",
            "107 391 Loss: 4.841 | Acc: 10.532% (1456/13824)\n",
            "108 391 Loss: 4.818 | Acc: 10.529% (1469/13952)\n",
            "109 391 Loss: 4.795 | Acc: 10.518% (1481/14080)\n",
            "110 391 Loss: 4.774 | Acc: 10.529% (1496/14208)\n",
            "111 391 Loss: 4.752 | Acc: 10.575% (1516/14336)\n",
            "112 391 Loss: 4.732 | Acc: 10.564% (1528/14464)\n",
            "113 391 Loss: 4.711 | Acc: 10.513% (1534/14592)\n",
            "114 391 Loss: 4.690 | Acc: 10.496% (1545/14720)\n",
            "115 391 Loss: 4.672 | Acc: 10.506% (1560/14848)\n",
            "116 391 Loss: 4.652 | Acc: 10.517% (1575/14976)\n",
            "117 391 Loss: 4.632 | Acc: 10.527% (1590/15104)\n",
            "118 391 Loss: 4.632 | Acc: 10.570% (1610/15232)\n",
            "119 391 Loss: 4.612 | Acc: 10.586% (1626/15360)\n",
            "120 391 Loss: 4.599 | Acc: 10.602% (1642/15488)\n",
            "121 391 Loss: 4.581 | Acc: 10.592% (1654/15616)\n",
            "122 391 Loss: 4.563 | Acc: 10.607% (1670/15744)\n",
            "123 391 Loss: 4.546 | Acc: 10.572% (1678/15872)\n",
            "124 391 Loss: 4.528 | Acc: 10.575% (1692/16000)\n",
            "125 391 Loss: 4.510 | Acc: 10.621% (1713/16128)\n",
            "126 391 Loss: 4.494 | Acc: 10.655% (1732/16256)\n",
            "127 391 Loss: 4.477 | Acc: 10.681% (1750/16384)\n",
            "128 391 Loss: 4.460 | Acc: 10.719% (1770/16512)\n",
            "129 391 Loss: 4.443 | Acc: 10.691% (1779/16640)\n",
            "130 391 Loss: 4.427 | Acc: 10.693% (1793/16768)\n",
            "131 391 Loss: 4.427 | Acc: 10.719% (1811/16896)\n",
            "132 391 Loss: 4.411 | Acc: 10.755% (1831/17024)\n",
            "133 391 Loss: 4.395 | Acc: 10.739% (1842/17152)\n",
            "134 391 Loss: 4.380 | Acc: 10.741% (1856/17280)\n",
            "135 391 Loss: 4.365 | Acc: 10.748% (1871/17408)\n",
            "136 391 Loss: 4.358 | Acc: 10.738% (1883/17536)\n",
            "137 391 Loss: 4.343 | Acc: 10.751% (1899/17664)\n",
            "138 391 Loss: 4.339 | Acc: 10.786% (1919/17792)\n",
            "139 391 Loss: 4.325 | Acc: 10.798% (1935/17920)\n",
            "140 391 Loss: 4.310 | Acc: 10.821% (1953/18048)\n",
            "141 391 Loss: 4.296 | Acc: 10.827% (1968/18176)\n",
            "142 391 Loss: 4.287 | Acc: 10.817% (1980/18304)\n",
            "143 391 Loss: 4.273 | Acc: 10.840% (1998/18432)\n",
            "144 391 Loss: 4.259 | Acc: 10.846% (2013/18560)\n",
            "145 391 Loss: 4.250 | Acc: 10.868% (2031/18688)\n",
            "146 391 Loss: 4.237 | Acc: 10.852% (2042/18816)\n",
            "147 391 Loss: 4.224 | Acc: 10.864% (2058/18944)\n",
            "148 391 Loss: 4.211 | Acc: 10.885% (2076/19072)\n",
            "149 391 Loss: 4.206 | Acc: 10.901% (2093/19200)\n",
            "150 391 Loss: 4.194 | Acc: 10.938% (2114/19328)\n",
            "151 391 Loss: 4.182 | Acc: 10.932% (2127/19456)\n",
            "152 391 Loss: 4.170 | Acc: 10.943% (2143/19584)\n",
            "153 391 Loss: 4.158 | Acc: 10.958% (2160/19712)\n",
            "154 391 Loss: 4.146 | Acc: 11.018% (2186/19840)\n",
            "155 391 Loss: 4.134 | Acc: 11.038% (2204/19968)\n",
            "156 391 Loss: 4.122 | Acc: 11.057% (2222/20096)\n",
            "157 391 Loss: 4.114 | Acc: 11.086% (2242/20224)\n",
            "158 391 Loss: 4.102 | Acc: 11.070% (2253/20352)\n",
            "159 391 Loss: 4.091 | Acc: 11.113% (2276/20480)\n",
            "160 391 Loss: 4.079 | Acc: 11.156% (2299/20608)\n",
            "161 391 Loss: 4.069 | Acc: 11.203% (2323/20736)\n",
            "162 391 Loss: 4.057 | Acc: 11.268% (2351/20864)\n",
            "163 391 Loss: 4.047 | Acc: 11.280% (2368/20992)\n",
            "164 391 Loss: 4.036 | Acc: 11.321% (2391/21120)\n",
            "165 391 Loss: 4.026 | Acc: 11.338% (2409/21248)\n",
            "166 391 Loss: 4.016 | Acc: 11.330% (2422/21376)\n",
            "167 391 Loss: 4.006 | Acc: 11.356% (2442/21504)\n",
            "168 391 Loss: 4.001 | Acc: 11.358% (2457/21632)\n",
            "169 391 Loss: 3.992 | Acc: 11.383% (2477/21760)\n",
            "170 391 Loss: 3.982 | Acc: 11.404% (2496/21888)\n",
            "171 391 Loss: 3.972 | Acc: 11.396% (2509/22016)\n",
            "172 391 Loss: 3.963 | Acc: 11.398% (2524/22144)\n",
            "173 391 Loss: 3.953 | Acc: 11.431% (2546/22272)\n",
            "174 391 Loss: 3.943 | Acc: 11.433% (2561/22400)\n",
            "175 391 Loss: 3.934 | Acc: 11.479% (2586/22528)\n",
            "176 391 Loss: 3.924 | Acc: 11.520% (2610/22656)\n",
            "177 391 Loss: 3.915 | Acc: 11.578% (2638/22784)\n",
            "178 391 Loss: 3.905 | Acc: 11.636% (2666/22912)\n",
            "179 391 Loss: 3.897 | Acc: 11.658% (2686/23040)\n",
            "180 391 Loss: 3.894 | Acc: 11.689% (2708/23168)\n",
            "181 391 Loss: 3.885 | Acc: 11.723% (2731/23296)\n",
            "182 391 Loss: 3.877 | Acc: 11.714% (2744/23424)\n",
            "183 391 Loss: 3.868 | Acc: 11.736% (2764/23552)\n",
            "184 391 Loss: 3.859 | Acc: 11.761% (2785/23680)\n",
            "185 391 Loss: 3.850 | Acc: 11.836% (2818/23808)\n",
            "186 391 Loss: 3.841 | Acc: 11.848% (2836/23936)\n",
            "187 391 Loss: 3.834 | Acc: 11.848% (2851/24064)\n",
            "188 391 Loss: 3.825 | Acc: 11.876% (2873/24192)\n",
            "189 391 Loss: 3.817 | Acc: 11.883% (2890/24320)\n",
            "190 391 Loss: 3.809 | Acc: 11.895% (2908/24448)\n",
            "191 391 Loss: 3.801 | Acc: 11.890% (2922/24576)\n",
            "192 391 Loss: 3.793 | Acc: 11.917% (2944/24704)\n",
            "193 391 Loss: 3.784 | Acc: 11.920% (2960/24832)\n",
            "194 391 Loss: 3.776 | Acc: 11.931% (2978/24960)\n",
            "195 391 Loss: 3.768 | Acc: 11.942% (2996/25088)\n",
            "196 391 Loss: 3.760 | Acc: 11.949% (3013/25216)\n",
            "197 391 Loss: 3.753 | Acc: 11.963% (3032/25344)\n",
            "198 391 Loss: 3.745 | Acc: 11.966% (3048/25472)\n",
            "199 391 Loss: 3.737 | Acc: 11.996% (3071/25600)\n",
            "200 391 Loss: 3.729 | Acc: 12.022% (3093/25728)\n",
            "201 391 Loss: 3.721 | Acc: 12.086% (3125/25856)\n",
            "202 391 Loss: 3.713 | Acc: 12.104% (3145/25984)\n",
            "203 391 Loss: 3.706 | Acc: 12.136% (3169/26112)\n",
            "204 391 Loss: 3.699 | Acc: 12.153% (3189/26240)\n",
            "205 391 Loss: 3.692 | Acc: 12.189% (3214/26368)\n",
            "206 391 Loss: 3.684 | Acc: 12.217% (3237/26496)\n",
            "207 391 Loss: 3.677 | Acc: 12.245% (3260/26624)\n",
            "208 391 Loss: 3.670 | Acc: 12.250% (3277/26752)\n",
            "209 391 Loss: 3.663 | Acc: 12.269% (3298/26880)\n",
            "210 391 Loss: 3.657 | Acc: 12.315% (3326/27008)\n",
            "211 391 Loss: 3.651 | Acc: 12.316% (3342/27136)\n",
            "212 391 Loss: 3.645 | Acc: 12.331% (3362/27264)\n",
            "213 391 Loss: 3.639 | Acc: 12.332% (3378/27392)\n",
            "214 391 Loss: 3.632 | Acc: 12.366% (3403/27520)\n",
            "215 391 Loss: 3.625 | Acc: 12.395% (3427/27648)\n",
            "216 391 Loss: 3.619 | Acc: 12.435% (3454/27776)\n",
            "217 391 Loss: 3.613 | Acc: 12.461% (3477/27904)\n",
            "218 391 Loss: 3.606 | Acc: 12.464% (3494/28032)\n",
            "219 391 Loss: 3.600 | Acc: 12.493% (3518/28160)\n",
            "220 391 Loss: 3.593 | Acc: 12.507% (3538/28288)\n",
            "221 391 Loss: 3.587 | Acc: 12.542% (3564/28416)\n",
            "222 391 Loss: 3.581 | Acc: 12.581% (3591/28544)\n",
            "223 391 Loss: 3.574 | Acc: 12.587% (3609/28672)\n",
            "224 391 Loss: 3.568 | Acc: 12.622% (3635/28800)\n",
            "225 391 Loss: 3.561 | Acc: 12.673% (3666/28928)\n",
            "226 391 Loss: 3.555 | Acc: 12.682% (3685/29056)\n",
            "227 391 Loss: 3.549 | Acc: 12.719% (3712/29184)\n",
            "228 391 Loss: 3.543 | Acc: 12.739% (3734/29312)\n",
            "229 391 Loss: 3.537 | Acc: 12.738% (3750/29440)\n",
            "230 391 Loss: 3.534 | Acc: 12.771% (3776/29568)\n",
            "231 391 Loss: 3.528 | Acc: 12.796% (3800/29696)\n",
            "232 391 Loss: 3.523 | Acc: 12.802% (3818/29824)\n",
            "233 391 Loss: 3.517 | Acc: 12.854% (3850/29952)\n",
            "234 391 Loss: 3.512 | Acc: 12.872% (3872/30080)\n",
            "235 391 Loss: 3.506 | Acc: 12.887% (3893/30208)\n",
            "236 391 Loss: 3.500 | Acc: 12.938% (3925/30336)\n",
            "237 391 Loss: 3.495 | Acc: 12.956% (3947/30464)\n",
            "238 391 Loss: 3.490 | Acc: 12.971% (3968/30592)\n",
            "239 391 Loss: 3.484 | Acc: 12.988% (3990/30720)\n",
            "240 391 Loss: 3.479 | Acc: 13.006% (4012/30848)\n",
            "241 391 Loss: 3.474 | Acc: 13.029% (4036/30976)\n",
            "242 391 Loss: 3.469 | Acc: 13.059% (4062/31104)\n",
            "243 391 Loss: 3.463 | Acc: 13.092% (4089/31232)\n",
            "244 391 Loss: 3.458 | Acc: 13.119% (4114/31360)\n",
            "245 391 Loss: 3.453 | Acc: 13.129% (4134/31488)\n",
            "246 391 Loss: 3.449 | Acc: 13.158% (4160/31616)\n",
            "247 391 Loss: 3.444 | Acc: 13.165% (4179/31744)\n",
            "248 391 Loss: 3.439 | Acc: 13.200% (4207/31872)\n",
            "249 391 Loss: 3.434 | Acc: 13.234% (4235/32000)\n",
            "250 391 Loss: 3.429 | Acc: 13.272% (4264/32128)\n",
            "251 391 Loss: 3.423 | Acc: 13.315% (4295/32256)\n",
            "252 391 Loss: 3.418 | Acc: 13.324% (4315/32384)\n",
            "253 391 Loss: 3.413 | Acc: 13.334% (4335/32512)\n",
            "254 391 Loss: 3.408 | Acc: 13.346% (4356/32640)\n",
            "255 391 Loss: 3.403 | Acc: 13.361% (4378/32768)\n",
            "256 391 Loss: 3.399 | Acc: 13.388% (4404/32896)\n",
            "257 391 Loss: 3.394 | Acc: 13.418% (4431/33024)\n",
            "258 391 Loss: 3.390 | Acc: 13.441% (4456/33152)\n",
            "259 391 Loss: 3.387 | Acc: 13.453% (4477/33280)\n",
            "260 391 Loss: 3.382 | Acc: 13.467% (4499/33408)\n",
            "261 391 Loss: 3.377 | Acc: 13.466% (4516/33536)\n",
            "262 391 Loss: 3.373 | Acc: 13.504% (4546/33664)\n",
            "263 391 Loss: 3.368 | Acc: 13.494% (4560/33792)\n",
            "264 391 Loss: 3.364 | Acc: 13.547% (4595/33920)\n",
            "265 391 Loss: 3.359 | Acc: 13.575% (4622/34048)\n",
            "266 391 Loss: 3.354 | Acc: 13.588% (4644/34176)\n",
            "267 391 Loss: 3.350 | Acc: 13.608% (4668/34304)\n",
            "268 391 Loss: 3.345 | Acc: 13.641% (4697/34432)\n",
            "269 391 Loss: 3.340 | Acc: 13.683% (4729/34560)\n",
            "270 391 Loss: 3.335 | Acc: 13.719% (4759/34688)\n",
            "271 391 Loss: 3.331 | Acc: 13.732% (4781/34816)\n",
            "272 391 Loss: 3.326 | Acc: 13.759% (4808/34944)\n",
            "273 391 Loss: 3.322 | Acc: 13.792% (4837/35072)\n",
            "274 391 Loss: 3.317 | Acc: 13.832% (4869/35200)\n",
            "275 391 Loss: 3.313 | Acc: 13.842% (4890/35328)\n",
            "276 391 Loss: 3.309 | Acc: 13.851% (4911/35456)\n",
            "277 391 Loss: 3.305 | Acc: 13.860% (4932/35584)\n",
            "278 391 Loss: 3.301 | Acc: 13.886% (4959/35712)\n",
            "279 391 Loss: 3.297 | Acc: 13.909% (4985/35840)\n",
            "280 391 Loss: 3.292 | Acc: 13.918% (5006/35968)\n",
            "281 391 Loss: 3.288 | Acc: 13.916% (5023/36096)\n",
            "282 391 Loss: 3.284 | Acc: 13.952% (5054/36224)\n",
            "283 391 Loss: 3.279 | Acc: 13.969% (5078/36352)\n",
            "284 391 Loss: 3.276 | Acc: 13.988% (5103/36480)\n",
            "285 391 Loss: 3.272 | Acc: 14.016% (5131/36608)\n",
            "286 391 Loss: 3.268 | Acc: 14.030% (5154/36736)\n",
            "287 391 Loss: 3.264 | Acc: 14.052% (5180/36864)\n",
            "288 391 Loss: 3.260 | Acc: 14.084% (5210/36992)\n",
            "289 391 Loss: 3.256 | Acc: 14.116% (5240/37120)\n",
            "290 391 Loss: 3.252 | Acc: 14.122% (5260/37248)\n",
            "291 391 Loss: 3.250 | Acc: 14.132% (5282/37376)\n",
            "292 391 Loss: 3.246 | Acc: 14.153% (5308/37504)\n",
            "293 391 Loss: 3.242 | Acc: 14.163% (5330/37632)\n",
            "294 391 Loss: 3.238 | Acc: 14.192% (5359/37760)\n",
            "295 391 Loss: 3.234 | Acc: 14.197% (5379/37888)\n",
            "296 391 Loss: 3.230 | Acc: 14.223% (5407/38016)\n",
            "297 391 Loss: 3.226 | Acc: 14.236% (5430/38144)\n",
            "298 391 Loss: 3.222 | Acc: 14.243% (5451/38272)\n",
            "299 391 Loss: 3.219 | Acc: 14.234% (5466/38400)\n",
            "300 391 Loss: 3.215 | Acc: 14.275% (5500/38528)\n",
            "301 391 Loss: 3.211 | Acc: 14.295% (5526/38656)\n",
            "302 391 Loss: 3.208 | Acc: 14.331% (5558/38784)\n",
            "303 391 Loss: 3.204 | Acc: 14.353% (5585/38912)\n",
            "304 391 Loss: 3.200 | Acc: 14.375% (5612/39040)\n",
            "305 391 Loss: 3.197 | Acc: 14.402% (5641/39168)\n",
            "306 391 Loss: 3.193 | Acc: 14.416% (5665/39296)\n",
            "307 391 Loss: 3.190 | Acc: 14.418% (5684/39424)\n",
            "308 391 Loss: 3.186 | Acc: 14.452% (5716/39552)\n",
            "309 391 Loss: 3.182 | Acc: 14.478% (5745/39680)\n",
            "310 391 Loss: 3.179 | Acc: 14.492% (5769/39808)\n",
            "311 391 Loss: 3.175 | Acc: 14.518% (5798/39936)\n",
            "312 391 Loss: 3.173 | Acc: 14.537% (5824/40064)\n",
            "313 391 Loss: 3.169 | Acc: 14.548% (5847/40192)\n",
            "314 391 Loss: 3.165 | Acc: 14.566% (5873/40320)\n",
            "315 391 Loss: 3.162 | Acc: 14.592% (5902/40448)\n",
            "316 391 Loss: 3.158 | Acc: 14.610% (5928/40576)\n",
            "317 391 Loss: 3.155 | Acc: 14.618% (5950/40704)\n",
            "318 391 Loss: 3.152 | Acc: 14.633% (5975/40832)\n",
            "319 391 Loss: 3.148 | Acc: 14.631% (5993/40960)\n",
            "320 391 Loss: 3.145 | Acc: 14.666% (6026/41088)\n",
            "321 391 Loss: 3.142 | Acc: 14.679% (6050/41216)\n",
            "322 391 Loss: 3.138 | Acc: 14.696% (6076/41344)\n",
            "323 391 Loss: 3.134 | Acc: 14.716% (6103/41472)\n",
            "324 391 Loss: 3.131 | Acc: 14.757% (6139/41600)\n",
            "325 391 Loss: 3.128 | Acc: 14.791% (6172/41728)\n",
            "326 391 Loss: 3.124 | Acc: 14.813% (6200/41856)\n",
            "327 391 Loss: 3.121 | Acc: 14.841% (6231/41984)\n",
            "328 391 Loss: 3.118 | Acc: 14.851% (6254/42112)\n",
            "329 391 Loss: 3.115 | Acc: 14.863% (6278/42240)\n",
            "330 391 Loss: 3.112 | Acc: 14.870% (6300/42368)\n",
            "331 391 Loss: 3.109 | Acc: 14.893% (6329/42496)\n",
            "332 391 Loss: 3.106 | Acc: 14.924% (6361/42624)\n",
            "333 391 Loss: 3.103 | Acc: 14.937% (6386/42752)\n",
            "334 391 Loss: 3.099 | Acc: 14.949% (6410/42880)\n",
            "335 391 Loss: 3.096 | Acc: 14.969% (6438/43008)\n",
            "336 391 Loss: 3.093 | Acc: 14.992% (6467/43136)\n",
            "337 391 Loss: 3.090 | Acc: 15.008% (6493/43264)\n",
            "338 391 Loss: 3.087 | Acc: 15.024% (6519/43392)\n",
            "339 391 Loss: 3.083 | Acc: 15.057% (6553/43520)\n",
            "340 391 Loss: 3.080 | Acc: 15.075% (6580/43648)\n",
            "341 391 Loss: 3.077 | Acc: 15.086% (6604/43776)\n",
            "342 391 Loss: 3.074 | Acc: 15.101% (6630/43904)\n",
            "343 391 Loss: 3.071 | Acc: 15.112% (6654/44032)\n",
            "344 391 Loss: 3.069 | Acc: 15.118% (6676/44160)\n",
            "345 391 Loss: 3.066 | Acc: 15.151% (6710/44288)\n",
            "346 391 Loss: 3.062 | Acc: 15.175% (6740/44416)\n",
            "347 391 Loss: 3.059 | Acc: 15.192% (6767/44544)\n",
            "348 391 Loss: 3.057 | Acc: 15.215% (6797/44672)\n",
            "349 391 Loss: 3.054 | Acc: 15.234% (6825/44800)\n",
            "350 391 Loss: 3.051 | Acc: 15.242% (6848/44928)\n",
            "351 391 Loss: 3.048 | Acc: 15.272% (6881/45056)\n",
            "352 391 Loss: 3.045 | Acc: 15.293% (6910/45184)\n",
            "353 391 Loss: 3.042 | Acc: 15.320% (6942/45312)\n",
            "354 391 Loss: 3.039 | Acc: 15.343% (6972/45440)\n",
            "355 391 Loss: 3.037 | Acc: 15.359% (6999/45568)\n",
            "356 391 Loss: 3.034 | Acc: 15.393% (7034/45696)\n",
            "357 391 Loss: 3.031 | Acc: 15.411% (7062/45824)\n",
            "358 391 Loss: 3.028 | Acc: 15.414% (7083/45952)\n",
            "359 391 Loss: 3.025 | Acc: 15.423% (7107/46080)\n",
            "360 391 Loss: 3.022 | Acc: 15.439% (7134/46208)\n",
            "361 391 Loss: 3.020 | Acc: 15.472% (7169/46336)\n",
            "362 391 Loss: 3.017 | Acc: 15.489% (7197/46464)\n",
            "363 391 Loss: 3.014 | Acc: 15.503% (7223/46592)\n",
            "364 391 Loss: 3.012 | Acc: 15.529% (7255/46720)\n",
            "365 391 Loss: 3.010 | Acc: 15.546% (7283/46848)\n",
            "366 391 Loss: 3.007 | Acc: 15.570% (7314/46976)\n",
            "367 391 Loss: 3.004 | Acc: 15.578% (7338/47104)\n",
            "368 391 Loss: 3.002 | Acc: 15.589% (7363/47232)\n",
            "369 391 Loss: 2.999 | Acc: 15.602% (7389/47360)\n",
            "370 391 Loss: 2.997 | Acc: 15.623% (7419/47488)\n",
            "371 391 Loss: 2.994 | Acc: 15.650% (7452/47616)\n",
            "372 391 Loss: 2.992 | Acc: 15.688% (7490/47744)\n",
            "373 391 Loss: 2.989 | Acc: 15.725% (7528/47872)\n",
            "374 391 Loss: 2.986 | Acc: 15.742% (7556/48000)\n",
            "375 391 Loss: 2.984 | Acc: 15.773% (7591/48128)\n",
            "376 391 Loss: 2.981 | Acc: 15.780% (7615/48256)\n",
            "377 391 Loss: 2.979 | Acc: 15.815% (7652/48384)\n",
            "378 391 Loss: 2.976 | Acc: 15.833% (7681/48512)\n",
            "379 391 Loss: 2.974 | Acc: 15.841% (7705/48640)\n",
            "380 391 Loss: 2.972 | Acc: 15.849% (7729/48768)\n",
            "381 391 Loss: 2.969 | Acc: 15.852% (7751/48896)\n",
            "382 391 Loss: 2.967 | Acc: 15.874% (7782/49024)\n",
            "383 391 Loss: 2.964 | Acc: 15.894% (7812/49152)\n",
            "384 391 Loss: 2.962 | Acc: 15.931% (7851/49280)\n",
            "385 391 Loss: 2.960 | Acc: 15.953% (7882/49408)\n",
            "386 391 Loss: 2.957 | Acc: 15.968% (7910/49536)\n",
            "387 391 Loss: 2.955 | Acc: 15.983% (7938/49664)\n",
            "388 391 Loss: 2.952 | Acc: 16.003% (7968/49792)\n",
            "389 391 Loss: 2.950 | Acc: 16.018% (7996/49920)\n",
            "390 391 Loss: 2.947 | Acc: 16.008% (8004/50000)\n",
            "0 100 Loss: 1.967 | Acc: 22.000% (22/100)\n",
            "1 100 Loss: 2.023 | Acc: 18.500% (37/200)\n",
            "2 100 Loss: 2.027 | Acc: 18.667% (56/300)\n",
            "3 100 Loss: 2.020 | Acc: 19.250% (77/400)\n",
            "4 100 Loss: 2.015 | Acc: 20.400% (102/500)\n",
            "5 100 Loss: 2.010 | Acc: 20.667% (124/600)\n",
            "6 100 Loss: 2.025 | Acc: 20.143% (141/700)\n",
            "7 100 Loss: 2.035 | Acc: 20.250% (162/800)\n",
            "8 100 Loss: 2.029 | Acc: 20.667% (186/900)\n",
            "9 100 Loss: 2.028 | Acc: 20.900% (209/1000)\n",
            "10 100 Loss: 2.027 | Acc: 20.545% (226/1100)\n",
            "11 100 Loss: 2.031 | Acc: 20.333% (244/1200)\n",
            "12 100 Loss: 2.034 | Acc: 20.385% (265/1300)\n",
            "13 100 Loss: 2.026 | Acc: 21.000% (294/1400)\n",
            "14 100 Loss: 2.020 | Acc: 21.133% (317/1500)\n",
            "15 100 Loss: 2.017 | Acc: 21.375% (342/1600)\n",
            "16 100 Loss: 2.017 | Acc: 21.529% (366/1700)\n",
            "17 100 Loss: 2.016 | Acc: 21.333% (384/1800)\n",
            "18 100 Loss: 2.016 | Acc: 21.737% (413/1900)\n",
            "19 100 Loss: 2.014 | Acc: 22.000% (440/2000)\n",
            "20 100 Loss: 2.017 | Acc: 21.714% (456/2100)\n",
            "21 100 Loss: 2.015 | Acc: 21.818% (480/2200)\n",
            "22 100 Loss: 2.015 | Acc: 22.130% (509/2300)\n",
            "23 100 Loss: 2.018 | Acc: 21.875% (525/2400)\n",
            "24 100 Loss: 2.014 | Acc: 21.760% (544/2500)\n",
            "25 100 Loss: 2.016 | Acc: 21.885% (569/2600)\n",
            "26 100 Loss: 2.015 | Acc: 21.889% (591/2700)\n",
            "27 100 Loss: 2.015 | Acc: 22.036% (617/2800)\n",
            "28 100 Loss: 2.015 | Acc: 21.966% (637/2900)\n",
            "29 100 Loss: 2.014 | Acc: 22.033% (661/3000)\n",
            "30 100 Loss: 2.012 | Acc: 22.258% (690/3100)\n",
            "31 100 Loss: 2.015 | Acc: 22.094% (707/3200)\n",
            "32 100 Loss: 2.015 | Acc: 22.121% (730/3300)\n",
            "33 100 Loss: 2.017 | Acc: 21.971% (747/3400)\n",
            "34 100 Loss: 2.019 | Acc: 21.886% (766/3500)\n",
            "35 100 Loss: 2.017 | Acc: 22.056% (794/3600)\n",
            "36 100 Loss: 2.017 | Acc: 22.081% (817/3700)\n",
            "37 100 Loss: 2.016 | Acc: 22.053% (838/3800)\n",
            "38 100 Loss: 2.018 | Acc: 22.103% (862/3900)\n",
            "39 100 Loss: 2.016 | Acc: 22.275% (891/4000)\n",
            "40 100 Loss: 2.014 | Acc: 22.415% (919/4100)\n",
            "41 100 Loss: 2.013 | Acc: 22.381% (940/4200)\n",
            "42 100 Loss: 2.013 | Acc: 22.465% (966/4300)\n",
            "43 100 Loss: 2.011 | Acc: 22.591% (994/4400)\n",
            "44 100 Loss: 2.014 | Acc: 22.578% (1016/4500)\n",
            "45 100 Loss: 2.014 | Acc: 22.609% (1040/4600)\n",
            "46 100 Loss: 2.013 | Acc: 22.574% (1061/4700)\n",
            "47 100 Loss: 2.013 | Acc: 22.562% (1083/4800)\n",
            "48 100 Loss: 2.014 | Acc: 22.510% (1103/4900)\n",
            "49 100 Loss: 2.013 | Acc: 22.520% (1126/5000)\n",
            "50 100 Loss: 2.013 | Acc: 22.392% (1142/5100)\n",
            "51 100 Loss: 2.013 | Acc: 22.346% (1162/5200)\n",
            "52 100 Loss: 2.012 | Acc: 22.377% (1186/5300)\n",
            "53 100 Loss: 2.015 | Acc: 22.259% (1202/5400)\n",
            "54 100 Loss: 2.016 | Acc: 22.182% (1220/5500)\n",
            "55 100 Loss: 2.016 | Acc: 22.018% (1233/5600)\n",
            "56 100 Loss: 2.017 | Acc: 22.140% (1262/5700)\n",
            "57 100 Loss: 2.016 | Acc: 22.103% (1282/5800)\n",
            "58 100 Loss: 2.016 | Acc: 22.068% (1302/5900)\n",
            "59 100 Loss: 2.016 | Acc: 22.117% (1327/6000)\n",
            "60 100 Loss: 2.016 | Acc: 22.098% (1348/6100)\n",
            "61 100 Loss: 2.016 | Acc: 22.129% (1372/6200)\n",
            "62 100 Loss: 2.015 | Acc: 22.222% (1400/6300)\n",
            "63 100 Loss: 2.014 | Acc: 22.172% (1419/6400)\n",
            "64 100 Loss: 2.016 | Acc: 22.154% (1440/6500)\n",
            "65 100 Loss: 2.016 | Acc: 22.091% (1458/6600)\n",
            "66 100 Loss: 2.015 | Acc: 22.075% (1479/6700)\n",
            "67 100 Loss: 2.015 | Acc: 22.118% (1504/6800)\n",
            "68 100 Loss: 2.015 | Acc: 22.029% (1520/6900)\n",
            "69 100 Loss: 2.016 | Acc: 22.014% (1541/7000)\n",
            "70 100 Loss: 2.017 | Acc: 21.986% (1561/7100)\n",
            "71 100 Loss: 2.016 | Acc: 22.042% (1587/7200)\n",
            "72 100 Loss: 2.015 | Acc: 22.068% (1611/7300)\n",
            "73 100 Loss: 2.014 | Acc: 22.068% (1633/7400)\n",
            "74 100 Loss: 2.014 | Acc: 22.107% (1658/7500)\n",
            "75 100 Loss: 2.014 | Acc: 22.092% (1679/7600)\n",
            "76 100 Loss: 2.014 | Acc: 22.052% (1698/7700)\n",
            "77 100 Loss: 2.015 | Acc: 22.051% (1720/7800)\n",
            "78 100 Loss: 2.015 | Acc: 21.911% (1731/7900)\n",
            "79 100 Loss: 2.016 | Acc: 21.812% (1745/8000)\n",
            "80 100 Loss: 2.018 | Acc: 21.790% (1765/8100)\n",
            "81 100 Loss: 2.017 | Acc: 21.854% (1792/8200)\n",
            "82 100 Loss: 2.017 | Acc: 21.795% (1809/8300)\n",
            "83 100 Loss: 2.017 | Acc: 21.798% (1831/8400)\n",
            "84 100 Loss: 2.018 | Acc: 21.788% (1852/8500)\n",
            "85 100 Loss: 2.018 | Acc: 21.651% (1862/8600)\n",
            "86 100 Loss: 2.018 | Acc: 21.632% (1882/8700)\n",
            "87 100 Loss: 2.017 | Acc: 21.602% (1901/8800)\n",
            "88 100 Loss: 2.017 | Acc: 21.618% (1924/8900)\n",
            "89 100 Loss: 2.017 | Acc: 21.589% (1943/9000)\n",
            "90 100 Loss: 2.017 | Acc: 21.538% (1960/9100)\n",
            "91 100 Loss: 2.017 | Acc: 21.609% (1988/9200)\n",
            "92 100 Loss: 2.016 | Acc: 21.602% (2009/9300)\n",
            "93 100 Loss: 2.017 | Acc: 21.585% (2029/9400)\n",
            "94 100 Loss: 2.017 | Acc: 21.537% (2046/9500)\n",
            "95 100 Loss: 2.017 | Acc: 21.490% (2063/9600)\n",
            "96 100 Loss: 2.016 | Acc: 21.505% (2086/9700)\n",
            "97 100 Loss: 2.016 | Acc: 21.531% (2110/9800)\n",
            "98 100 Loss: 2.015 | Acc: 21.535% (2132/9900)\n",
            "99 100 Loss: 2.015 | Acc: 21.530% (2153/10000)\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 2.037 | Acc: 18.750% (24/128)\n",
            "1 391 Loss: 1.981 | Acc: 20.703% (53/256)\n",
            "2 391 Loss: 1.992 | Acc: 21.094% (81/384)\n",
            "3 391 Loss: 1.982 | Acc: 21.875% (112/512)\n",
            "4 391 Loss: 1.983 | Acc: 22.656% (145/640)\n",
            "5 391 Loss: 1.994 | Acc: 22.656% (174/768)\n",
            "6 391 Loss: 1.990 | Acc: 22.768% (204/896)\n",
            "7 391 Loss: 2.005 | Acc: 22.461% (230/1024)\n",
            "8 391 Loss: 1.996 | Acc: 22.049% (254/1152)\n",
            "9 391 Loss: 2.003 | Acc: 22.109% (283/1280)\n",
            "10 391 Loss: 2.000 | Acc: 22.301% (314/1408)\n",
            "11 391 Loss: 2.004 | Acc: 22.656% (348/1536)\n",
            "12 391 Loss: 2.006 | Acc: 22.536% (375/1664)\n",
            "13 391 Loss: 2.004 | Acc: 22.656% (406/1792)\n",
            "14 391 Loss: 2.004 | Acc: 22.552% (433/1920)\n",
            "15 391 Loss: 2.013 | Acc: 22.656% (464/2048)\n",
            "16 391 Loss: 2.012 | Acc: 22.840% (497/2176)\n",
            "17 391 Loss: 2.013 | Acc: 22.483% (518/2304)\n",
            "18 391 Loss: 2.013 | Acc: 22.533% (548/2432)\n",
            "19 391 Loss: 2.011 | Acc: 22.539% (577/2560)\n",
            "20 391 Loss: 2.009 | Acc: 22.545% (606/2688)\n",
            "21 391 Loss: 2.014 | Acc: 22.656% (638/2816)\n",
            "22 391 Loss: 2.011 | Acc: 22.690% (668/2944)\n",
            "23 391 Loss: 2.012 | Acc: 22.428% (689/3072)\n",
            "24 391 Loss: 2.010 | Acc: 22.688% (726/3200)\n",
            "25 391 Loss: 2.008 | Acc: 22.596% (752/3328)\n",
            "26 391 Loss: 2.008 | Acc: 22.772% (787/3456)\n",
            "27 391 Loss: 2.010 | Acc: 22.573% (809/3584)\n",
            "28 391 Loss: 2.007 | Acc: 22.791% (846/3712)\n",
            "29 391 Loss: 2.008 | Acc: 22.604% (868/3840)\n",
            "30 391 Loss: 2.008 | Acc: 22.656% (899/3968)\n",
            "31 391 Loss: 2.005 | Acc: 22.681% (929/4096)\n",
            "32 391 Loss: 2.005 | Acc: 22.680% (958/4224)\n",
            "33 391 Loss: 2.005 | Acc: 22.564% (982/4352)\n",
            "34 391 Loss: 2.004 | Acc: 22.567% (1011/4480)\n",
            "35 391 Loss: 2.007 | Acc: 22.591% (1041/4608)\n",
            "36 391 Loss: 2.006 | Acc: 22.614% (1071/4736)\n",
            "37 391 Loss: 2.006 | Acc: 22.718% (1105/4864)\n",
            "38 391 Loss: 2.005 | Acc: 22.837% (1140/4992)\n",
            "39 391 Loss: 2.006 | Acc: 22.871% (1171/5120)\n",
            "40 391 Loss: 2.004 | Acc: 22.885% (1201/5248)\n",
            "41 391 Loss: 2.003 | Acc: 22.954% (1234/5376)\n",
            "42 391 Loss: 2.003 | Acc: 22.892% (1260/5504)\n",
            "43 391 Loss: 2.004 | Acc: 22.976% (1294/5632)\n",
            "44 391 Loss: 2.003 | Acc: 23.038% (1327/5760)\n",
            "45 391 Loss: 2.003 | Acc: 23.132% (1362/5888)\n",
            "46 391 Loss: 2.002 | Acc: 23.072% (1388/6016)\n",
            "47 391 Loss: 2.002 | Acc: 23.128% (1421/6144)\n",
            "48 391 Loss: 2.003 | Acc: 23.151% (1452/6272)\n",
            "49 391 Loss: 2.002 | Acc: 23.188% (1484/6400)\n",
            "50 391 Loss: 2.003 | Acc: 23.146% (1511/6528)\n",
            "51 391 Loss: 1.999 | Acc: 23.302% (1551/6656)\n",
            "52 391 Loss: 1.998 | Acc: 23.305% (1581/6784)\n",
            "53 391 Loss: 1.996 | Acc: 23.293% (1610/6912)\n",
            "54 391 Loss: 1.997 | Acc: 23.395% (1647/7040)\n",
            "55 391 Loss: 1.995 | Acc: 23.424% (1679/7168)\n",
            "56 391 Loss: 1.994 | Acc: 23.465% (1712/7296)\n",
            "57 391 Loss: 1.994 | Acc: 23.478% (1743/7424)\n",
            "58 391 Loss: 1.993 | Acc: 23.490% (1774/7552)\n",
            "59 391 Loss: 1.991 | Acc: 23.620% (1814/7680)\n",
            "60 391 Loss: 1.991 | Acc: 23.732% (1853/7808)\n",
            "61 391 Loss: 1.991 | Acc: 23.753% (1885/7936)\n",
            "62 391 Loss: 1.990 | Acc: 23.760% (1916/8064)\n",
            "63 391 Loss: 1.990 | Acc: 23.730% (1944/8192)\n",
            "64 391 Loss: 1.992 | Acc: 23.762% (1977/8320)\n",
            "65 391 Loss: 1.992 | Acc: 23.698% (2002/8448)\n",
            "66 391 Loss: 1.991 | Acc: 23.787% (2040/8576)\n",
            "67 391 Loss: 1.990 | Acc: 23.782% (2070/8704)\n",
            "68 391 Loss: 1.990 | Acc: 23.788% (2101/8832)\n",
            "69 391 Loss: 1.991 | Acc: 23.694% (2123/8960)\n",
            "70 391 Loss: 1.991 | Acc: 23.658% (2150/9088)\n",
            "71 391 Loss: 1.990 | Acc: 23.676% (2182/9216)\n",
            "72 391 Loss: 1.989 | Acc: 23.780% (2222/9344)\n",
            "73 391 Loss: 1.989 | Acc: 23.786% (2253/9472)\n",
            "74 391 Loss: 1.987 | Acc: 23.760% (2281/9600)\n",
            "75 391 Loss: 1.986 | Acc: 23.869% (2322/9728)\n",
            "76 391 Loss: 1.985 | Acc: 23.894% (2355/9856)\n",
            "77 391 Loss: 1.984 | Acc: 23.888% (2385/9984)\n",
            "78 391 Loss: 1.983 | Acc: 24.021% (2429/10112)\n",
            "79 391 Loss: 1.982 | Acc: 24.023% (2460/10240)\n",
            "80 391 Loss: 1.980 | Acc: 24.113% (2500/10368)\n",
            "81 391 Loss: 1.979 | Acc: 24.200% (2540/10496)\n",
            "82 391 Loss: 1.977 | Acc: 24.238% (2575/10624)\n",
            "83 391 Loss: 1.978 | Acc: 24.172% (2599/10752)\n",
            "84 391 Loss: 1.978 | Acc: 24.136% (2626/10880)\n",
            "85 391 Loss: 1.979 | Acc: 24.128% (2656/11008)\n",
            "86 391 Loss: 1.979 | Acc: 24.147% (2689/11136)\n",
            "87 391 Loss: 1.979 | Acc: 24.174% (2723/11264)\n",
            "88 391 Loss: 1.978 | Acc: 24.184% (2755/11392)\n",
            "89 391 Loss: 1.978 | Acc: 24.149% (2782/11520)\n",
            "90 391 Loss: 1.979 | Acc: 24.133% (2811/11648)\n",
            "91 391 Loss: 1.979 | Acc: 24.219% (2852/11776)\n",
            "92 391 Loss: 1.978 | Acc: 24.311% (2894/11904)\n",
            "93 391 Loss: 1.977 | Acc: 24.343% (2929/12032)\n",
            "94 391 Loss: 1.975 | Acc: 24.375% (2964/12160)\n",
            "95 391 Loss: 1.975 | Acc: 24.390% (2997/12288)\n",
            "96 391 Loss: 1.974 | Acc: 24.388% (3028/12416)\n",
            "97 391 Loss: 1.973 | Acc: 24.442% (3066/12544)\n",
            "98 391 Loss: 1.972 | Acc: 24.511% (3106/12672)\n",
            "99 391 Loss: 1.971 | Acc: 24.586% (3147/12800)\n",
            "100 391 Loss: 1.972 | Acc: 24.551% (3174/12928)\n",
            "101 391 Loss: 1.971 | Acc: 24.563% (3207/13056)\n",
            "102 391 Loss: 1.971 | Acc: 24.530% (3234/13184)\n",
            "103 391 Loss: 1.971 | Acc: 24.602% (3275/13312)\n",
            "104 391 Loss: 1.972 | Acc: 24.606% (3307/13440)\n",
            "105 391 Loss: 1.971 | Acc: 24.624% (3341/13568)\n",
            "106 391 Loss: 1.971 | Acc: 24.642% (3375/13696)\n",
            "107 391 Loss: 1.971 | Acc: 24.674% (3411/13824)\n",
            "108 391 Loss: 1.971 | Acc: 24.685% (3444/13952)\n",
            "109 391 Loss: 1.971 | Acc: 24.709% (3479/14080)\n",
            "110 391 Loss: 1.970 | Acc: 24.761% (3518/14208)\n",
            "111 391 Loss: 1.970 | Acc: 24.791% (3554/14336)\n",
            "112 391 Loss: 1.970 | Acc: 24.758% (3581/14464)\n",
            "113 391 Loss: 1.971 | Acc: 24.705% (3605/14592)\n",
            "114 391 Loss: 1.971 | Acc: 24.715% (3638/14720)\n",
            "115 391 Loss: 1.971 | Acc: 24.731% (3672/14848)\n",
            "116 391 Loss: 1.971 | Acc: 24.753% (3707/14976)\n",
            "117 391 Loss: 1.970 | Acc: 24.729% (3735/15104)\n",
            "118 391 Loss: 1.970 | Acc: 24.777% (3774/15232)\n",
            "119 391 Loss: 1.970 | Acc: 24.818% (3812/15360)\n",
            "120 391 Loss: 1.971 | Acc: 24.813% (3843/15488)\n",
            "121 391 Loss: 1.971 | Acc: 24.795% (3872/15616)\n",
            "122 391 Loss: 1.970 | Acc: 24.854% (3913/15744)\n",
            "123 391 Loss: 1.969 | Acc: 24.868% (3947/15872)\n",
            "124 391 Loss: 1.970 | Acc: 24.856% (3977/16000)\n",
            "125 391 Loss: 1.970 | Acc: 24.857% (4009/16128)\n",
            "126 391 Loss: 1.969 | Acc: 24.865% (4042/16256)\n",
            "127 391 Loss: 1.969 | Acc: 24.896% (4079/16384)\n",
            "128 391 Loss: 1.969 | Acc: 24.921% (4115/16512)\n",
            "129 391 Loss: 1.968 | Acc: 24.904% (4144/16640)\n",
            "130 391 Loss: 1.968 | Acc: 24.869% (4170/16768)\n",
            "131 391 Loss: 1.967 | Acc: 24.852% (4199/16896)\n",
            "132 391 Loss: 1.967 | Acc: 24.824% (4226/17024)\n",
            "133 391 Loss: 1.967 | Acc: 24.866% (4265/17152)\n",
            "134 391 Loss: 1.966 | Acc: 24.873% (4298/17280)\n",
            "135 391 Loss: 1.966 | Acc: 24.885% (4332/17408)\n",
            "136 391 Loss: 1.966 | Acc: 24.914% (4369/17536)\n",
            "137 391 Loss: 1.967 | Acc: 24.853% (4390/17664)\n",
            "138 391 Loss: 1.967 | Acc: 24.927% (4435/17792)\n",
            "139 391 Loss: 1.967 | Acc: 24.927% (4467/17920)\n",
            "140 391 Loss: 1.968 | Acc: 24.917% (4497/18048)\n",
            "141 391 Loss: 1.967 | Acc: 24.934% (4532/18176)\n",
            "142 391 Loss: 1.967 | Acc: 24.907% (4559/18304)\n",
            "143 391 Loss: 1.966 | Acc: 24.940% (4597/18432)\n",
            "144 391 Loss: 1.966 | Acc: 24.941% (4629/18560)\n",
            "145 391 Loss: 1.967 | Acc: 24.941% (4661/18688)\n",
            "146 391 Loss: 1.968 | Acc: 24.915% (4688/18816)\n",
            "147 391 Loss: 1.968 | Acc: 24.926% (4722/18944)\n",
            "148 391 Loss: 1.967 | Acc: 24.879% (4745/19072)\n",
            "149 391 Loss: 1.967 | Acc: 24.839% (4769/19200)\n",
            "150 391 Loss: 1.968 | Acc: 24.840% (4801/19328)\n",
            "151 391 Loss: 1.968 | Acc: 24.836% (4832/19456)\n",
            "152 391 Loss: 1.967 | Acc: 24.842% (4865/19584)\n",
            "153 391 Loss: 1.966 | Acc: 24.924% (4913/19712)\n",
            "154 391 Loss: 1.966 | Acc: 24.955% (4951/19840)\n",
            "155 391 Loss: 1.966 | Acc: 24.935% (4979/19968)\n",
            "156 391 Loss: 1.966 | Acc: 24.980% (5020/20096)\n",
            "157 391 Loss: 1.965 | Acc: 25.005% (5057/20224)\n",
            "158 391 Loss: 1.965 | Acc: 24.971% (5082/20352)\n",
            "159 391 Loss: 1.964 | Acc: 25.029% (5126/20480)\n",
            "160 391 Loss: 1.965 | Acc: 25.034% (5159/20608)\n",
            "161 391 Loss: 1.964 | Acc: 25.029% (5190/20736)\n",
            "162 391 Loss: 1.964 | Acc: 24.986% (5213/20864)\n",
            "163 391 Loss: 1.965 | Acc: 24.976% (5243/20992)\n",
            "164 391 Loss: 1.965 | Acc: 24.967% (5273/21120)\n",
            "165 391 Loss: 1.964 | Acc: 24.995% (5311/21248)\n",
            "166 391 Loss: 1.964 | Acc: 25.023% (5349/21376)\n",
            "167 391 Loss: 1.965 | Acc: 24.995% (5375/21504)\n",
            "168 391 Loss: 1.965 | Acc: 24.977% (5403/21632)\n",
            "169 391 Loss: 1.965 | Acc: 24.991% (5438/21760)\n",
            "170 391 Loss: 1.965 | Acc: 24.995% (5471/21888)\n",
            "171 391 Loss: 1.964 | Acc: 25.023% (5509/22016)\n",
            "172 391 Loss: 1.964 | Acc: 25.032% (5543/22144)\n",
            "173 391 Loss: 1.964 | Acc: 25.031% (5575/22272)\n",
            "174 391 Loss: 1.964 | Acc: 25.036% (5608/22400)\n",
            "175 391 Loss: 1.963 | Acc: 25.053% (5644/22528)\n",
            "176 391 Loss: 1.963 | Acc: 25.044% (5674/22656)\n",
            "177 391 Loss: 1.963 | Acc: 25.075% (5713/22784)\n",
            "178 391 Loss: 1.962 | Acc: 25.118% (5755/22912)\n",
            "179 391 Loss: 1.962 | Acc: 25.135% (5791/23040)\n",
            "180 391 Loss: 1.962 | Acc: 25.117% (5819/23168)\n",
            "181 391 Loss: 1.962 | Acc: 25.112% (5850/23296)\n",
            "182 391 Loss: 1.962 | Acc: 25.145% (5890/23424)\n",
            "183 391 Loss: 1.961 | Acc: 25.183% (5931/23552)\n",
            "184 391 Loss: 1.960 | Acc: 25.224% (5973/23680)\n",
            "185 391 Loss: 1.959 | Acc: 25.260% (6014/23808)\n",
            "186 391 Loss: 1.959 | Acc: 25.263% (6047/23936)\n",
            "187 391 Loss: 1.959 | Acc: 25.270% (6081/24064)\n",
            "188 391 Loss: 1.959 | Acc: 25.277% (6115/24192)\n",
            "189 391 Loss: 1.960 | Acc: 25.251% (6141/24320)\n",
            "190 391 Loss: 1.959 | Acc: 25.270% (6178/24448)\n",
            "191 391 Loss: 1.959 | Acc: 25.264% (6209/24576)\n",
            "192 391 Loss: 1.958 | Acc: 25.344% (6261/24704)\n",
            "193 391 Loss: 1.958 | Acc: 25.370% (6300/24832)\n",
            "194 391 Loss: 1.957 | Acc: 25.397% (6339/24960)\n",
            "195 391 Loss: 1.957 | Acc: 25.426% (6379/25088)\n",
            "196 391 Loss: 1.957 | Acc: 25.401% (6405/25216)\n",
            "197 391 Loss: 1.958 | Acc: 25.351% (6425/25344)\n",
            "198 391 Loss: 1.958 | Acc: 25.345% (6456/25472)\n",
            "199 391 Loss: 1.957 | Acc: 25.371% (6495/25600)\n",
            "200 391 Loss: 1.958 | Acc: 25.346% (6521/25728)\n",
            "201 391 Loss: 1.957 | Acc: 25.348% (6554/25856)\n",
            "202 391 Loss: 1.957 | Acc: 25.362% (6590/25984)\n",
            "203 391 Loss: 1.957 | Acc: 25.345% (6618/26112)\n",
            "204 391 Loss: 1.957 | Acc: 25.339% (6649/26240)\n",
            "205 391 Loss: 1.957 | Acc: 25.356% (6686/26368)\n",
            "206 391 Loss: 1.957 | Acc: 25.366% (6721/26496)\n",
            "207 391 Loss: 1.956 | Acc: 25.357% (6751/26624)\n",
            "208 391 Loss: 1.956 | Acc: 25.374% (6788/26752)\n",
            "209 391 Loss: 1.955 | Acc: 25.391% (6825/26880)\n",
            "210 391 Loss: 1.955 | Acc: 25.363% (6850/27008)\n",
            "211 391 Loss: 1.955 | Acc: 25.372% (6885/27136)\n",
            "212 391 Loss: 1.955 | Acc: 25.400% (6925/27264)\n",
            "213 391 Loss: 1.954 | Acc: 25.431% (6966/27392)\n",
            "214 391 Loss: 1.954 | Acc: 25.436% (7000/27520)\n",
            "215 391 Loss: 1.954 | Acc: 25.452% (7037/27648)\n",
            "216 391 Loss: 1.954 | Acc: 25.446% (7068/27776)\n",
            "217 391 Loss: 1.953 | Acc: 25.462% (7105/27904)\n",
            "218 391 Loss: 1.953 | Acc: 25.482% (7143/28032)\n",
            "219 391 Loss: 1.952 | Acc: 25.472% (7173/28160)\n",
            "220 391 Loss: 1.952 | Acc: 25.467% (7204/28288)\n",
            "221 391 Loss: 1.952 | Acc: 25.486% (7242/28416)\n",
            "222 391 Loss: 1.952 | Acc: 25.497% (7278/28544)\n",
            "223 391 Loss: 1.951 | Acc: 25.537% (7322/28672)\n",
            "224 391 Loss: 1.950 | Acc: 25.573% (7365/28800)\n",
            "225 391 Loss: 1.950 | Acc: 25.553% (7392/28928)\n",
            "226 391 Loss: 1.950 | Acc: 25.571% (7430/29056)\n",
            "227 391 Loss: 1.949 | Acc: 25.562% (7460/29184)\n",
            "228 391 Loss: 1.949 | Acc: 25.566% (7494/29312)\n",
            "229 391 Loss: 1.950 | Acc: 25.554% (7523/29440)\n",
            "230 391 Loss: 1.950 | Acc: 25.572% (7561/29568)\n",
            "231 391 Loss: 1.949 | Acc: 25.556% (7589/29696)\n",
            "232 391 Loss: 1.949 | Acc: 25.550% (7620/29824)\n",
            "233 391 Loss: 1.949 | Acc: 25.564% (7657/29952)\n",
            "234 391 Loss: 1.949 | Acc: 25.549% (7685/30080)\n",
            "235 391 Loss: 1.949 | Acc: 25.563% (7722/30208)\n",
            "236 391 Loss: 1.948 | Acc: 25.593% (7764/30336)\n",
            "237 391 Loss: 1.948 | Acc: 25.601% (7799/30464)\n",
            "238 391 Loss: 1.948 | Acc: 25.641% (7844/30592)\n",
            "239 391 Loss: 1.948 | Acc: 25.658% (7882/30720)\n",
            "240 391 Loss: 1.947 | Acc: 25.655% (7914/30848)\n",
            "241 391 Loss: 1.947 | Acc: 25.707% (7963/30976)\n",
            "242 391 Loss: 1.946 | Acc: 25.736% (8005/31104)\n",
            "243 391 Loss: 1.946 | Acc: 25.772% (8049/31232)\n",
            "244 391 Loss: 1.945 | Acc: 25.804% (8092/31360)\n",
            "245 391 Loss: 1.946 | Acc: 25.803% (8125/31488)\n",
            "246 391 Loss: 1.946 | Acc: 25.807% (8159/31616)\n",
            "247 391 Loss: 1.945 | Acc: 25.822% (8197/31744)\n",
            "248 391 Loss: 1.944 | Acc: 25.863% (8243/31872)\n",
            "249 391 Loss: 1.944 | Acc: 25.859% (8275/32000)\n",
            "250 391 Loss: 1.944 | Acc: 25.884% (8316/32128)\n",
            "251 391 Loss: 1.943 | Acc: 25.908% (8357/32256)\n",
            "252 391 Loss: 1.943 | Acc: 25.929% (8397/32384)\n",
            "253 391 Loss: 1.943 | Acc: 25.914% (8425/32512)\n",
            "254 391 Loss: 1.942 | Acc: 25.944% (8468/32640)\n",
            "255 391 Loss: 1.942 | Acc: 25.955% (8505/32768)\n",
            "256 391 Loss: 1.942 | Acc: 25.961% (8540/32896)\n",
            "257 391 Loss: 1.942 | Acc: 25.933% (8564/33024)\n",
            "258 391 Loss: 1.941 | Acc: 25.950% (8603/33152)\n",
            "259 391 Loss: 1.941 | Acc: 25.953% (8637/33280)\n",
            "260 391 Loss: 1.940 | Acc: 25.967% (8675/33408)\n",
            "261 391 Loss: 1.940 | Acc: 25.984% (8714/33536)\n",
            "262 391 Loss: 1.940 | Acc: 25.986% (8748/33664)\n",
            "263 391 Loss: 1.939 | Acc: 25.994% (8784/33792)\n",
            "264 391 Loss: 1.939 | Acc: 26.011% (8823/33920)\n",
            "265 391 Loss: 1.939 | Acc: 26.034% (8864/34048)\n",
            "266 391 Loss: 1.939 | Acc: 26.021% (8893/34176)\n",
            "267 391 Loss: 1.939 | Acc: 26.014% (8924/34304)\n",
            "268 391 Loss: 1.939 | Acc: 26.025% (8961/34432)\n",
            "269 391 Loss: 1.938 | Acc: 26.047% (9002/34560)\n",
            "270 391 Loss: 1.938 | Acc: 26.061% (9040/34688)\n",
            "271 391 Loss: 1.938 | Acc: 26.080% (9080/34816)\n",
            "272 391 Loss: 1.938 | Acc: 26.090% (9117/34944)\n",
            "273 391 Loss: 1.937 | Acc: 26.109% (9157/35072)\n",
            "274 391 Loss: 1.937 | Acc: 26.125% (9196/35200)\n",
            "275 391 Loss: 1.937 | Acc: 26.149% (9238/35328)\n",
            "276 391 Loss: 1.936 | Acc: 26.162% (9276/35456)\n",
            "277 391 Loss: 1.936 | Acc: 26.180% (9316/35584)\n",
            "278 391 Loss: 1.935 | Acc: 26.204% (9358/35712)\n",
            "279 391 Loss: 1.935 | Acc: 26.222% (9398/35840)\n",
            "280 391 Loss: 1.934 | Acc: 26.234% (9436/35968)\n",
            "281 391 Loss: 1.934 | Acc: 26.247% (9474/36096)\n",
            "282 391 Loss: 1.934 | Acc: 26.259% (9512/36224)\n",
            "283 391 Loss: 1.933 | Acc: 26.274% (9551/36352)\n",
            "284 391 Loss: 1.933 | Acc: 26.275% (9585/36480)\n",
            "285 391 Loss: 1.933 | Acc: 26.306% (9630/36608)\n",
            "286 391 Loss: 1.932 | Acc: 26.318% (9668/36736)\n",
            "287 391 Loss: 1.932 | Acc: 26.348% (9713/36864)\n",
            "288 391 Loss: 1.931 | Acc: 26.373% (9756/36992)\n",
            "289 391 Loss: 1.931 | Acc: 26.406% (9802/37120)\n",
            "290 391 Loss: 1.930 | Acc: 26.436% (9847/37248)\n",
            "291 391 Loss: 1.930 | Acc: 26.461% (9890/37376)\n",
            "292 391 Loss: 1.929 | Acc: 26.456% (9922/37504)\n",
            "293 391 Loss: 1.929 | Acc: 26.462% (9958/37632)\n",
            "294 391 Loss: 1.929 | Acc: 26.486% (10001/37760)\n",
            "295 391 Loss: 1.928 | Acc: 26.470% (10029/37888)\n",
            "296 391 Loss: 1.928 | Acc: 26.484% (10068/38016)\n",
            "297 391 Loss: 1.928 | Acc: 26.494% (10106/38144)\n",
            "298 391 Loss: 1.927 | Acc: 26.531% (10154/38272)\n",
            "299 391 Loss: 1.927 | Acc: 26.570% (10203/38400)\n",
            "300 391 Loss: 1.927 | Acc: 26.565% (10235/38528)\n",
            "301 391 Loss: 1.926 | Acc: 26.578% (10274/38656)\n",
            "302 391 Loss: 1.926 | Acc: 26.588% (10312/38784)\n",
            "303 391 Loss: 1.926 | Acc: 26.591% (10347/38912)\n",
            "304 391 Loss: 1.926 | Acc: 26.570% (10373/39040)\n",
            "305 391 Loss: 1.926 | Acc: 26.568% (10406/39168)\n",
            "306 391 Loss: 1.926 | Acc: 26.578% (10444/39296)\n",
            "307 391 Loss: 1.926 | Acc: 26.580% (10479/39424)\n",
            "308 391 Loss: 1.925 | Acc: 26.605% (10523/39552)\n",
            "309 391 Loss: 1.925 | Acc: 26.633% (10568/39680)\n",
            "310 391 Loss: 1.925 | Acc: 26.665% (10615/39808)\n",
            "311 391 Loss: 1.925 | Acc: 26.653% (10644/39936)\n",
            "312 391 Loss: 1.924 | Acc: 26.690% (10693/40064)\n",
            "313 391 Loss: 1.923 | Acc: 26.687% (10726/40192)\n",
            "314 391 Loss: 1.923 | Acc: 26.716% (10772/40320)\n",
            "315 391 Loss: 1.923 | Acc: 26.731% (10812/40448)\n",
            "316 391 Loss: 1.923 | Acc: 26.737% (10849/40576)\n",
            "317 391 Loss: 1.923 | Acc: 26.786% (10903/40704)\n",
            "318 391 Loss: 1.923 | Acc: 26.810% (10947/40832)\n",
            "319 391 Loss: 1.923 | Acc: 26.812% (10982/40960)\n",
            "320 391 Loss: 1.922 | Acc: 26.833% (11025/41088)\n",
            "321 391 Loss: 1.922 | Acc: 26.863% (11072/41216)\n",
            "322 391 Loss: 1.921 | Acc: 26.858% (11104/41344)\n",
            "323 391 Loss: 1.921 | Acc: 26.861% (11140/41472)\n",
            "324 391 Loss: 1.921 | Acc: 26.875% (11180/41600)\n",
            "325 391 Loss: 1.920 | Acc: 26.891% (11221/41728)\n",
            "326 391 Loss: 1.920 | Acc: 26.916% (11266/41856)\n",
            "327 391 Loss: 1.920 | Acc: 26.941% (11311/41984)\n",
            "328 391 Loss: 1.919 | Acc: 26.947% (11348/42112)\n",
            "329 391 Loss: 1.919 | Acc: 26.963% (11389/42240)\n",
            "330 391 Loss: 1.919 | Acc: 26.983% (11432/42368)\n",
            "331 391 Loss: 1.919 | Acc: 26.972% (11462/42496)\n",
            "332 391 Loss: 1.919 | Acc: 26.971% (11496/42624)\n",
            "333 391 Loss: 1.918 | Acc: 26.998% (11542/42752)\n",
            "334 391 Loss: 1.918 | Acc: 27.006% (11580/42880)\n",
            "335 391 Loss: 1.918 | Acc: 27.030% (11625/43008)\n",
            "336 391 Loss: 1.917 | Acc: 27.052% (11669/43136)\n",
            "337 391 Loss: 1.917 | Acc: 27.055% (11705/43264)\n",
            "338 391 Loss: 1.916 | Acc: 27.083% (11752/43392)\n",
            "339 391 Loss: 1.916 | Acc: 27.080% (11785/43520)\n",
            "340 391 Loss: 1.916 | Acc: 27.103% (11830/43648)\n",
            "341 391 Loss: 1.916 | Acc: 27.115% (11870/43776)\n",
            "342 391 Loss: 1.915 | Acc: 27.143% (11917/43904)\n",
            "343 391 Loss: 1.915 | Acc: 27.169% (11963/44032)\n",
            "344 391 Loss: 1.914 | Acc: 27.174% (12000/44160)\n",
            "345 391 Loss: 1.914 | Acc: 27.177% (12036/44288)\n",
            "346 391 Loss: 1.914 | Acc: 27.209% (12085/44416)\n",
            "347 391 Loss: 1.913 | Acc: 27.234% (12131/44544)\n",
            "348 391 Loss: 1.912 | Acc: 27.256% (12176/44672)\n",
            "349 391 Loss: 1.912 | Acc: 27.270% (12217/44800)\n",
            "350 391 Loss: 1.912 | Acc: 27.270% (12252/44928)\n",
            "351 391 Loss: 1.912 | Acc: 27.262% (12283/45056)\n",
            "352 391 Loss: 1.912 | Acc: 27.269% (12321/45184)\n",
            "353 391 Loss: 1.911 | Acc: 27.291% (12366/45312)\n",
            "354 391 Loss: 1.912 | Acc: 27.298% (12404/45440)\n",
            "355 391 Loss: 1.912 | Acc: 27.302% (12441/45568)\n",
            "356 391 Loss: 1.912 | Acc: 27.317% (12483/45696)\n",
            "357 391 Loss: 1.911 | Acc: 27.318% (12518/45824)\n",
            "358 391 Loss: 1.911 | Acc: 27.329% (12558/45952)\n",
            "359 391 Loss: 1.911 | Acc: 27.337% (12597/46080)\n",
            "360 391 Loss: 1.910 | Acc: 27.352% (12639/46208)\n",
            "361 391 Loss: 1.910 | Acc: 27.363% (12679/46336)\n",
            "362 391 Loss: 1.910 | Acc: 27.372% (12718/46464)\n",
            "363 391 Loss: 1.910 | Acc: 27.385% (12759/46592)\n",
            "364 391 Loss: 1.909 | Acc: 27.399% (12801/46720)\n",
            "365 391 Loss: 1.909 | Acc: 27.406% (12839/46848)\n",
            "366 391 Loss: 1.909 | Acc: 27.433% (12887/46976)\n",
            "367 391 Loss: 1.909 | Acc: 27.433% (12922/47104)\n",
            "368 391 Loss: 1.908 | Acc: 27.439% (12960/47232)\n",
            "369 391 Loss: 1.908 | Acc: 27.454% (13002/47360)\n",
            "370 391 Loss: 1.908 | Acc: 27.464% (13042/47488)\n",
            "371 391 Loss: 1.908 | Acc: 27.453% (13072/47616)\n",
            "372 391 Loss: 1.908 | Acc: 27.436% (13099/47744)\n",
            "373 391 Loss: 1.908 | Acc: 27.429% (13131/47872)\n",
            "374 391 Loss: 1.907 | Acc: 27.444% (13173/48000)\n",
            "375 391 Loss: 1.907 | Acc: 27.475% (13223/48128)\n",
            "376 391 Loss: 1.907 | Acc: 27.483% (13262/48256)\n",
            "377 391 Loss: 1.906 | Acc: 27.486% (13299/48384)\n",
            "378 391 Loss: 1.906 | Acc: 27.519% (13350/48512)\n",
            "379 391 Loss: 1.906 | Acc: 27.537% (13394/48640)\n",
            "380 391 Loss: 1.906 | Acc: 27.541% (13431/48768)\n",
            "381 391 Loss: 1.905 | Acc: 27.552% (13472/48896)\n",
            "382 391 Loss: 1.905 | Acc: 27.580% (13521/49024)\n",
            "383 391 Loss: 1.905 | Acc: 27.574% (13553/49152)\n",
            "384 391 Loss: 1.905 | Acc: 27.579% (13591/49280)\n",
            "385 391 Loss: 1.904 | Acc: 27.593% (13633/49408)\n",
            "386 391 Loss: 1.904 | Acc: 27.608% (13676/49536)\n",
            "387 391 Loss: 1.904 | Acc: 27.608% (13711/49664)\n",
            "388 391 Loss: 1.904 | Acc: 27.609% (13747/49792)\n",
            "389 391 Loss: 1.903 | Acc: 27.622% (13789/49920)\n",
            "390 391 Loss: 1.903 | Acc: 27.634% (13817/50000)\n",
            "0 100 Loss: 1.712 | Acc: 37.000% (37/100)\n",
            "1 100 Loss: 1.700 | Acc: 36.500% (73/200)\n",
            "2 100 Loss: 1.700 | Acc: 36.667% (110/300)\n",
            "3 100 Loss: 1.712 | Acc: 37.750% (151/400)\n",
            "4 100 Loss: 1.726 | Acc: 37.200% (186/500)\n",
            "5 100 Loss: 1.741 | Acc: 37.167% (223/600)\n",
            "6 100 Loss: 1.775 | Acc: 36.143% (253/700)\n",
            "7 100 Loss: 1.779 | Acc: 35.625% (285/800)\n",
            "8 100 Loss: 1.773 | Acc: 35.222% (317/900)\n",
            "9 100 Loss: 1.760 | Acc: 35.700% (357/1000)\n",
            "10 100 Loss: 1.758 | Acc: 35.545% (391/1100)\n",
            "11 100 Loss: 1.753 | Acc: 35.500% (426/1200)\n",
            "12 100 Loss: 1.759 | Acc: 35.077% (456/1300)\n",
            "13 100 Loss: 1.757 | Acc: 34.857% (488/1400)\n",
            "14 100 Loss: 1.746 | Acc: 35.000% (525/1500)\n",
            "15 100 Loss: 1.747 | Acc: 35.188% (563/1600)\n",
            "16 100 Loss: 1.745 | Acc: 35.412% (602/1700)\n",
            "17 100 Loss: 1.744 | Acc: 35.389% (637/1800)\n",
            "18 100 Loss: 1.745 | Acc: 35.789% (680/1900)\n",
            "19 100 Loss: 1.748 | Acc: 35.600% (712/2000)\n",
            "20 100 Loss: 1.748 | Acc: 35.619% (748/2100)\n",
            "21 100 Loss: 1.752 | Acc: 35.455% (780/2200)\n",
            "22 100 Loss: 1.748 | Acc: 35.609% (819/2300)\n",
            "23 100 Loss: 1.749 | Acc: 35.583% (854/2400)\n",
            "24 100 Loss: 1.746 | Acc: 36.000% (900/2500)\n",
            "25 100 Loss: 1.749 | Acc: 35.769% (930/2600)\n",
            "26 100 Loss: 1.748 | Acc: 35.593% (961/2700)\n",
            "27 100 Loss: 1.746 | Acc: 35.571% (996/2800)\n",
            "28 100 Loss: 1.751 | Acc: 35.345% (1025/2900)\n",
            "29 100 Loss: 1.747 | Acc: 35.233% (1057/3000)\n",
            "30 100 Loss: 1.741 | Acc: 35.452% (1099/3100)\n",
            "31 100 Loss: 1.744 | Acc: 35.281% (1129/3200)\n",
            "32 100 Loss: 1.745 | Acc: 35.333% (1166/3300)\n",
            "33 100 Loss: 1.744 | Acc: 35.265% (1199/3400)\n",
            "34 100 Loss: 1.745 | Acc: 35.343% (1237/3500)\n",
            "35 100 Loss: 1.742 | Acc: 35.444% (1276/3600)\n",
            "36 100 Loss: 1.741 | Acc: 35.459% (1312/3700)\n",
            "37 100 Loss: 1.740 | Acc: 35.421% (1346/3800)\n",
            "38 100 Loss: 1.741 | Acc: 35.333% (1378/3900)\n",
            "39 100 Loss: 1.741 | Acc: 35.400% (1416/4000)\n",
            "40 100 Loss: 1.742 | Acc: 35.244% (1445/4100)\n",
            "41 100 Loss: 1.742 | Acc: 35.286% (1482/4200)\n",
            "42 100 Loss: 1.742 | Acc: 35.163% (1512/4300)\n",
            "43 100 Loss: 1.737 | Acc: 35.409% (1558/4400)\n",
            "44 100 Loss: 1.740 | Acc: 35.333% (1590/4500)\n",
            "45 100 Loss: 1.739 | Acc: 35.304% (1624/4600)\n",
            "46 100 Loss: 1.738 | Acc: 35.340% (1661/4700)\n",
            "47 100 Loss: 1.738 | Acc: 35.417% (1700/4800)\n",
            "48 100 Loss: 1.737 | Acc: 35.469% (1738/4900)\n",
            "49 100 Loss: 1.735 | Acc: 35.540% (1777/5000)\n",
            "50 100 Loss: 1.737 | Acc: 35.549% (1813/5100)\n",
            "51 100 Loss: 1.737 | Acc: 35.577% (1850/5200)\n",
            "52 100 Loss: 1.737 | Acc: 35.604% (1887/5300)\n",
            "53 100 Loss: 1.738 | Acc: 35.648% (1925/5400)\n",
            "54 100 Loss: 1.742 | Acc: 35.527% (1954/5500)\n",
            "55 100 Loss: 1.745 | Acc: 35.429% (1984/5600)\n",
            "56 100 Loss: 1.748 | Acc: 35.351% (2015/5700)\n",
            "57 100 Loss: 1.745 | Acc: 35.534% (2061/5800)\n",
            "58 100 Loss: 1.744 | Acc: 35.475% (2093/5900)\n",
            "59 100 Loss: 1.745 | Acc: 35.400% (2124/6000)\n",
            "60 100 Loss: 1.746 | Acc: 35.246% (2150/6100)\n",
            "61 100 Loss: 1.746 | Acc: 35.226% (2184/6200)\n",
            "62 100 Loss: 1.746 | Acc: 35.222% (2219/6300)\n",
            "63 100 Loss: 1.745 | Acc: 35.281% (2258/6400)\n",
            "64 100 Loss: 1.746 | Acc: 35.185% (2287/6500)\n",
            "65 100 Loss: 1.747 | Acc: 35.167% (2321/6600)\n",
            "66 100 Loss: 1.746 | Acc: 35.194% (2358/6700)\n",
            "67 100 Loss: 1.746 | Acc: 35.044% (2383/6800)\n",
            "68 100 Loss: 1.745 | Acc: 35.087% (2421/6900)\n",
            "69 100 Loss: 1.746 | Acc: 35.029% (2452/7000)\n",
            "70 100 Loss: 1.748 | Acc: 34.986% (2484/7100)\n",
            "71 100 Loss: 1.746 | Acc: 35.042% (2523/7200)\n",
            "72 100 Loss: 1.746 | Acc: 35.055% (2559/7300)\n",
            "73 100 Loss: 1.745 | Acc: 35.081% (2596/7400)\n",
            "74 100 Loss: 1.746 | Acc: 35.000% (2625/7500)\n",
            "75 100 Loss: 1.746 | Acc: 34.947% (2656/7600)\n",
            "76 100 Loss: 1.744 | Acc: 34.961% (2692/7700)\n",
            "77 100 Loss: 1.745 | Acc: 34.974% (2728/7800)\n",
            "78 100 Loss: 1.745 | Acc: 35.025% (2767/7900)\n",
            "79 100 Loss: 1.746 | Acc: 35.000% (2800/8000)\n",
            "80 100 Loss: 1.745 | Acc: 35.037% (2838/8100)\n",
            "81 100 Loss: 1.746 | Acc: 34.915% (2863/8200)\n",
            "82 100 Loss: 1.747 | Acc: 34.831% (2891/8300)\n",
            "83 100 Loss: 1.747 | Acc: 34.881% (2930/8400)\n",
            "84 100 Loss: 1.748 | Acc: 34.847% (2962/8500)\n",
            "85 100 Loss: 1.748 | Acc: 34.826% (2995/8600)\n",
            "86 100 Loss: 1.748 | Acc: 34.839% (3031/8700)\n",
            "87 100 Loss: 1.748 | Acc: 34.875% (3069/8800)\n",
            "88 100 Loss: 1.748 | Acc: 34.831% (3100/8900)\n",
            "89 100 Loss: 1.748 | Acc: 34.878% (3139/9000)\n",
            "90 100 Loss: 1.748 | Acc: 34.802% (3167/9100)\n",
            "91 100 Loss: 1.749 | Acc: 34.804% (3202/9200)\n",
            "92 100 Loss: 1.748 | Acc: 34.839% (3240/9300)\n",
            "93 100 Loss: 1.749 | Acc: 34.840% (3275/9400)\n",
            "94 100 Loss: 1.749 | Acc: 34.842% (3310/9500)\n",
            "95 100 Loss: 1.748 | Acc: 34.823% (3343/9600)\n",
            "96 100 Loss: 1.746 | Acc: 34.928% (3388/9700)\n",
            "97 100 Loss: 1.747 | Acc: 34.878% (3418/9800)\n",
            "98 100 Loss: 1.746 | Acc: 34.848% (3450/9900)\n",
            "99 100 Loss: 1.747 | Acc: 34.840% (3484/10000)\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 1.871 | Acc: 28.125% (36/128)\n",
            "1 391 Loss: 1.801 | Acc: 33.203% (85/256)\n",
            "2 391 Loss: 1.815 | Acc: 32.812% (126/384)\n",
            "3 391 Loss: 1.823 | Acc: 33.398% (171/512)\n",
            "4 391 Loss: 1.809 | Acc: 33.125% (212/640)\n",
            "5 391 Loss: 1.793 | Acc: 33.724% (259/768)\n",
            "6 391 Loss: 1.801 | Acc: 33.705% (302/896)\n",
            "7 391 Loss: 1.803 | Acc: 33.691% (345/1024)\n",
            "8 391 Loss: 1.799 | Acc: 33.941% (391/1152)\n",
            "9 391 Loss: 1.803 | Acc: 33.750% (432/1280)\n",
            "10 391 Loss: 1.800 | Acc: 33.665% (474/1408)\n",
            "11 391 Loss: 1.804 | Acc: 33.333% (512/1536)\n",
            "12 391 Loss: 1.802 | Acc: 32.873% (547/1664)\n",
            "13 391 Loss: 1.793 | Acc: 33.315% (597/1792)\n",
            "14 391 Loss: 1.802 | Acc: 33.177% (637/1920)\n",
            "15 391 Loss: 1.797 | Acc: 33.350% (683/2048)\n",
            "16 391 Loss: 1.803 | Acc: 33.134% (721/2176)\n",
            "17 391 Loss: 1.798 | Acc: 33.203% (765/2304)\n",
            "18 391 Loss: 1.797 | Acc: 33.224% (808/2432)\n",
            "19 391 Loss: 1.800 | Acc: 33.047% (846/2560)\n",
            "20 391 Loss: 1.802 | Acc: 33.036% (888/2688)\n",
            "21 391 Loss: 1.804 | Acc: 32.955% (928/2816)\n",
            "22 391 Loss: 1.806 | Acc: 32.643% (961/2944)\n",
            "23 391 Loss: 1.806 | Acc: 32.520% (999/3072)\n",
            "24 391 Loss: 1.805 | Acc: 32.594% (1043/3200)\n",
            "25 391 Loss: 1.805 | Acc: 32.572% (1084/3328)\n",
            "26 391 Loss: 1.807 | Acc: 32.581% (1126/3456)\n",
            "27 391 Loss: 1.803 | Acc: 32.785% (1175/3584)\n",
            "28 391 Loss: 1.802 | Acc: 33.028% (1226/3712)\n",
            "29 391 Loss: 1.800 | Acc: 32.839% (1261/3840)\n",
            "30 391 Loss: 1.799 | Acc: 32.913% (1306/3968)\n",
            "31 391 Loss: 1.798 | Acc: 32.812% (1344/4096)\n",
            "32 391 Loss: 1.799 | Acc: 32.623% (1378/4224)\n",
            "33 391 Loss: 1.799 | Acc: 32.744% (1425/4352)\n",
            "34 391 Loss: 1.794 | Acc: 32.835% (1471/4480)\n",
            "35 391 Loss: 1.796 | Acc: 32.878% (1515/4608)\n",
            "36 391 Loss: 1.791 | Acc: 33.045% (1565/4736)\n",
            "37 391 Loss: 1.790 | Acc: 33.059% (1608/4864)\n",
            "38 391 Loss: 1.795 | Acc: 32.953% (1645/4992)\n",
            "39 391 Loss: 1.792 | Acc: 33.047% (1692/5120)\n",
            "40 391 Loss: 1.792 | Acc: 33.022% (1733/5248)\n",
            "41 391 Loss: 1.793 | Acc: 33.017% (1775/5376)\n",
            "42 391 Loss: 1.792 | Acc: 33.176% (1826/5504)\n",
            "43 391 Loss: 1.788 | Acc: 33.310% (1876/5632)\n",
            "44 391 Loss: 1.786 | Acc: 33.385% (1923/5760)\n",
            "45 391 Loss: 1.785 | Acc: 33.492% (1972/5888)\n",
            "46 391 Loss: 1.786 | Acc: 33.577% (2020/6016)\n",
            "47 391 Loss: 1.788 | Acc: 33.496% (2058/6144)\n",
            "48 391 Loss: 1.789 | Acc: 33.339% (2091/6272)\n",
            "49 391 Loss: 1.793 | Acc: 33.250% (2128/6400)\n",
            "50 391 Loss: 1.791 | Acc: 33.272% (2172/6528)\n",
            "51 391 Loss: 1.791 | Acc: 33.143% (2206/6656)\n",
            "52 391 Loss: 1.790 | Acc: 33.137% (2248/6784)\n",
            "53 391 Loss: 1.790 | Acc: 33.073% (2286/6912)\n",
            "54 391 Loss: 1.788 | Acc: 33.097% (2330/7040)\n",
            "55 391 Loss: 1.789 | Acc: 33.119% (2374/7168)\n",
            "56 391 Loss: 1.789 | Acc: 33.018% (2409/7296)\n",
            "57 391 Loss: 1.790 | Acc: 32.974% (2448/7424)\n",
            "58 391 Loss: 1.790 | Acc: 32.958% (2489/7552)\n",
            "59 391 Loss: 1.789 | Acc: 32.930% (2529/7680)\n",
            "60 391 Loss: 1.789 | Acc: 33.017% (2578/7808)\n",
            "61 391 Loss: 1.788 | Acc: 33.065% (2624/7936)\n",
            "62 391 Loss: 1.787 | Acc: 33.160% (2674/8064)\n",
            "63 391 Loss: 1.788 | Acc: 33.105% (2712/8192)\n",
            "64 391 Loss: 1.789 | Acc: 32.945% (2741/8320)\n",
            "65 391 Loss: 1.790 | Acc: 32.884% (2778/8448)\n",
            "66 391 Loss: 1.789 | Acc: 32.894% (2821/8576)\n",
            "67 391 Loss: 1.788 | Acc: 32.916% (2865/8704)\n",
            "68 391 Loss: 1.787 | Acc: 32.960% (2911/8832)\n",
            "69 391 Loss: 1.788 | Acc: 33.069% (2963/8960)\n",
            "70 391 Loss: 1.785 | Acc: 33.077% (3006/9088)\n",
            "71 391 Loss: 1.786 | Acc: 33.073% (3048/9216)\n",
            "72 391 Loss: 1.786 | Acc: 33.091% (3092/9344)\n",
            "73 391 Loss: 1.785 | Acc: 33.108% (3136/9472)\n",
            "74 391 Loss: 1.784 | Acc: 33.135% (3181/9600)\n",
            "75 391 Loss: 1.785 | Acc: 33.090% (3219/9728)\n",
            "76 391 Loss: 1.785 | Acc: 33.097% (3262/9856)\n",
            "77 391 Loss: 1.783 | Acc: 33.103% (3305/9984)\n",
            "78 391 Loss: 1.783 | Acc: 33.089% (3346/10112)\n",
            "79 391 Loss: 1.785 | Acc: 32.998% (3379/10240)\n",
            "80 391 Loss: 1.786 | Acc: 32.919% (3413/10368)\n",
            "81 391 Loss: 1.786 | Acc: 32.927% (3456/10496)\n",
            "82 391 Loss: 1.785 | Acc: 32.963% (3502/10624)\n",
            "83 391 Loss: 1.786 | Acc: 32.933% (3541/10752)\n",
            "84 391 Loss: 1.786 | Acc: 32.950% (3585/10880)\n",
            "85 391 Loss: 1.786 | Acc: 32.985% (3631/11008)\n",
            "86 391 Loss: 1.787 | Acc: 33.010% (3676/11136)\n",
            "87 391 Loss: 1.786 | Acc: 32.999% (3717/11264)\n",
            "88 391 Loss: 1.784 | Acc: 33.050% (3765/11392)\n",
            "89 391 Loss: 1.784 | Acc: 33.064% (3809/11520)\n",
            "90 391 Loss: 1.784 | Acc: 33.027% (3847/11648)\n",
            "91 391 Loss: 1.783 | Acc: 33.076% (3895/11776)\n",
            "92 391 Loss: 1.782 | Acc: 33.056% (3935/11904)\n",
            "93 391 Loss: 1.783 | Acc: 33.037% (3975/12032)\n",
            "94 391 Loss: 1.782 | Acc: 33.067% (4021/12160)\n",
            "95 391 Loss: 1.783 | Acc: 33.049% (4061/12288)\n",
            "96 391 Loss: 1.783 | Acc: 33.014% (4099/12416)\n",
            "97 391 Loss: 1.785 | Acc: 32.980% (4137/12544)\n",
            "98 391 Loss: 1.786 | Acc: 32.939% (4174/12672)\n",
            "99 391 Loss: 1.786 | Acc: 32.977% (4221/12800)\n",
            "100 391 Loss: 1.786 | Acc: 32.959% (4261/12928)\n",
            "101 391 Loss: 1.786 | Acc: 32.943% (4301/13056)\n",
            "102 391 Loss: 1.787 | Acc: 32.949% (4344/13184)\n",
            "103 391 Loss: 1.788 | Acc: 32.918% (4382/13312)\n",
            "104 391 Loss: 1.788 | Acc: 32.887% (4420/13440)\n",
            "105 391 Loss: 1.788 | Acc: 32.879% (4461/13568)\n",
            "106 391 Loss: 1.787 | Acc: 32.915% (4508/13696)\n",
            "107 391 Loss: 1.787 | Acc: 32.950% (4555/13824)\n",
            "108 391 Loss: 1.787 | Acc: 32.927% (4594/13952)\n",
            "109 391 Loss: 1.787 | Acc: 32.933% (4637/14080)\n",
            "110 391 Loss: 1.787 | Acc: 32.855% (4668/14208)\n",
            "111 391 Loss: 1.787 | Acc: 32.924% (4720/14336)\n",
            "112 391 Loss: 1.788 | Acc: 32.868% (4754/14464)\n",
            "113 391 Loss: 1.788 | Acc: 32.867% (4796/14592)\n",
            "114 391 Loss: 1.788 | Acc: 32.887% (4841/14720)\n",
            "115 391 Loss: 1.788 | Acc: 32.893% (4884/14848)\n",
            "116 391 Loss: 1.787 | Acc: 32.913% (4929/14976)\n",
            "117 391 Loss: 1.786 | Acc: 32.945% (4976/15104)\n",
            "118 391 Loss: 1.787 | Acc: 32.878% (5008/15232)\n",
            "119 391 Loss: 1.786 | Acc: 32.897% (5053/15360)\n",
            "120 391 Loss: 1.785 | Acc: 32.903% (5096/15488)\n",
            "121 391 Loss: 1.785 | Acc: 32.960% (5147/15616)\n",
            "122 391 Loss: 1.785 | Acc: 32.971% (5191/15744)\n",
            "123 391 Loss: 1.784 | Acc: 33.014% (5240/15872)\n",
            "124 391 Loss: 1.785 | Acc: 33.025% (5284/16000)\n",
            "125 391 Loss: 1.785 | Acc: 33.017% (5325/16128)\n",
            "126 391 Loss: 1.784 | Acc: 33.077% (5377/16256)\n",
            "127 391 Loss: 1.784 | Acc: 33.087% (5421/16384)\n",
            "128 391 Loss: 1.784 | Acc: 33.103% (5466/16512)\n",
            "129 391 Loss: 1.783 | Acc: 33.143% (5515/16640)\n",
            "130 391 Loss: 1.784 | Acc: 33.141% (5557/16768)\n",
            "131 391 Loss: 1.783 | Acc: 33.114% (5595/16896)\n",
            "132 391 Loss: 1.783 | Acc: 33.112% (5637/17024)\n",
            "133 391 Loss: 1.783 | Acc: 33.116% (5680/17152)\n",
            "134 391 Loss: 1.782 | Acc: 33.171% (5732/17280)\n",
            "135 391 Loss: 1.782 | Acc: 33.157% (5772/17408)\n",
            "136 391 Loss: 1.781 | Acc: 33.160% (5815/17536)\n",
            "137 391 Loss: 1.781 | Acc: 33.180% (5861/17664)\n",
            "138 391 Loss: 1.780 | Acc: 33.212% (5909/17792)\n",
            "139 391 Loss: 1.780 | Acc: 33.225% (5954/17920)\n",
            "140 391 Loss: 1.779 | Acc: 33.261% (6003/18048)\n",
            "141 391 Loss: 1.779 | Acc: 33.247% (6043/18176)\n",
            "142 391 Loss: 1.780 | Acc: 33.206% (6078/18304)\n",
            "143 391 Loss: 1.779 | Acc: 33.290% (6136/18432)\n",
            "144 391 Loss: 1.778 | Acc: 33.346% (6189/18560)\n",
            "145 391 Loss: 1.777 | Acc: 33.401% (6242/18688)\n",
            "146 391 Loss: 1.775 | Acc: 33.461% (6296/18816)\n",
            "147 391 Loss: 1.775 | Acc: 33.451% (6337/18944)\n",
            "148 391 Loss: 1.775 | Acc: 33.452% (6380/19072)\n",
            "149 391 Loss: 1.774 | Acc: 33.479% (6428/19200)\n",
            "150 391 Loss: 1.773 | Acc: 33.526% (6480/19328)\n",
            "151 391 Loss: 1.773 | Acc: 33.517% (6521/19456)\n",
            "152 391 Loss: 1.773 | Acc: 33.492% (6559/19584)\n",
            "153 391 Loss: 1.773 | Acc: 33.508% (6605/19712)\n",
            "154 391 Loss: 1.774 | Acc: 33.508% (6648/19840)\n",
            "155 391 Loss: 1.773 | Acc: 33.534% (6696/19968)\n",
            "156 391 Loss: 1.772 | Acc: 33.549% (6742/20096)\n",
            "157 391 Loss: 1.773 | Acc: 33.549% (6785/20224)\n",
            "158 391 Loss: 1.772 | Acc: 33.579% (6834/20352)\n",
            "159 391 Loss: 1.772 | Acc: 33.550% (6871/20480)\n",
            "160 391 Loss: 1.772 | Acc: 33.565% (6917/20608)\n",
            "161 391 Loss: 1.771 | Acc: 33.632% (6974/20736)\n",
            "162 391 Loss: 1.770 | Acc: 33.670% (7025/20864)\n",
            "163 391 Loss: 1.771 | Acc: 33.641% (7062/20992)\n",
            "164 391 Loss: 1.771 | Acc: 33.613% (7099/21120)\n",
            "165 391 Loss: 1.771 | Acc: 33.603% (7140/21248)\n",
            "166 391 Loss: 1.771 | Acc: 33.622% (7187/21376)\n",
            "167 391 Loss: 1.771 | Acc: 33.622% (7230/21504)\n",
            "168 391 Loss: 1.770 | Acc: 33.663% (7282/21632)\n",
            "169 391 Loss: 1.769 | Acc: 33.704% (7334/21760)\n",
            "170 391 Loss: 1.770 | Acc: 33.731% (7383/21888)\n",
            "171 391 Loss: 1.769 | Acc: 33.789% (7439/22016)\n",
            "172 391 Loss: 1.770 | Acc: 33.783% (7481/22144)\n",
            "173 391 Loss: 1.769 | Acc: 33.800% (7528/22272)\n",
            "174 391 Loss: 1.769 | Acc: 33.772% (7565/22400)\n",
            "175 391 Loss: 1.767 | Acc: 33.825% (7620/22528)\n",
            "176 391 Loss: 1.768 | Acc: 33.797% (7657/22656)\n",
            "177 391 Loss: 1.767 | Acc: 33.826% (7707/22784)\n",
            "178 391 Loss: 1.767 | Acc: 33.838% (7753/22912)\n",
            "179 391 Loss: 1.766 | Acc: 33.885% (7807/23040)\n",
            "180 391 Loss: 1.767 | Acc: 33.892% (7852/23168)\n",
            "181 391 Loss: 1.766 | Acc: 33.907% (7899/23296)\n",
            "182 391 Loss: 1.766 | Acc: 33.940% (7950/23424)\n",
            "183 391 Loss: 1.765 | Acc: 33.942% (7994/23552)\n",
            "184 391 Loss: 1.765 | Acc: 33.944% (8038/23680)\n",
            "185 391 Loss: 1.765 | Acc: 33.926% (8077/23808)\n",
            "186 391 Loss: 1.764 | Acc: 33.920% (8119/23936)\n",
            "187 391 Loss: 1.764 | Acc: 33.905% (8159/24064)\n",
            "188 391 Loss: 1.763 | Acc: 33.920% (8206/24192)\n",
            "189 391 Loss: 1.764 | Acc: 33.927% (8251/24320)\n",
            "190 391 Loss: 1.763 | Acc: 33.946% (8299/24448)\n",
            "191 391 Loss: 1.763 | Acc: 33.944% (8342/24576)\n",
            "192 391 Loss: 1.763 | Acc: 33.926% (8381/24704)\n",
            "193 391 Loss: 1.763 | Acc: 33.916% (8422/24832)\n",
            "194 391 Loss: 1.762 | Acc: 33.914% (8465/24960)\n",
            "195 391 Loss: 1.762 | Acc: 33.952% (8518/25088)\n",
            "196 391 Loss: 1.762 | Acc: 33.947% (8560/25216)\n",
            "197 391 Loss: 1.762 | Acc: 33.961% (8607/25344)\n",
            "198 391 Loss: 1.761 | Acc: 34.002% (8661/25472)\n",
            "199 391 Loss: 1.761 | Acc: 34.031% (8712/25600)\n",
            "200 391 Loss: 1.761 | Acc: 34.033% (8756/25728)\n",
            "201 391 Loss: 1.760 | Acc: 34.027% (8798/25856)\n",
            "202 391 Loss: 1.760 | Acc: 34.006% (8836/25984)\n",
            "203 391 Loss: 1.760 | Acc: 33.988% (8875/26112)\n",
            "204 391 Loss: 1.760 | Acc: 34.024% (8928/26240)\n",
            "205 391 Loss: 1.759 | Acc: 34.060% (8981/26368)\n",
            "206 391 Loss: 1.759 | Acc: 34.069% (9027/26496)\n",
            "207 391 Loss: 1.759 | Acc: 34.059% (9068/26624)\n",
            "208 391 Loss: 1.758 | Acc: 34.083% (9118/26752)\n",
            "209 391 Loss: 1.758 | Acc: 34.070% (9158/26880)\n",
            "210 391 Loss: 1.757 | Acc: 34.079% (9204/27008)\n",
            "211 391 Loss: 1.757 | Acc: 34.095% (9252/27136)\n",
            "212 391 Loss: 1.757 | Acc: 34.129% (9305/27264)\n",
            "213 391 Loss: 1.756 | Acc: 34.130% (9349/27392)\n",
            "214 391 Loss: 1.756 | Acc: 34.150% (9398/27520)\n",
            "215 391 Loss: 1.756 | Acc: 34.158% (9444/27648)\n",
            "216 391 Loss: 1.756 | Acc: 34.170% (9491/27776)\n",
            "217 391 Loss: 1.757 | Acc: 34.138% (9526/27904)\n",
            "218 391 Loss: 1.757 | Acc: 34.150% (9573/28032)\n",
            "219 391 Loss: 1.757 | Acc: 34.158% (9619/28160)\n",
            "220 391 Loss: 1.756 | Acc: 34.177% (9668/28288)\n",
            "221 391 Loss: 1.756 | Acc: 34.178% (9712/28416)\n",
            "222 391 Loss: 1.756 | Acc: 34.203% (9763/28544)\n",
            "223 391 Loss: 1.756 | Acc: 34.232% (9815/28672)\n",
            "224 391 Loss: 1.755 | Acc: 34.240% (9861/28800)\n",
            "225 391 Loss: 1.755 | Acc: 34.244% (9906/28928)\n",
            "226 391 Loss: 1.754 | Acc: 34.241% (9949/29056)\n",
            "227 391 Loss: 1.755 | Acc: 34.207% (9983/29184)\n",
            "228 391 Loss: 1.754 | Acc: 34.259% (10042/29312)\n",
            "229 391 Loss: 1.754 | Acc: 34.263% (10087/29440)\n",
            "230 391 Loss: 1.754 | Acc: 34.287% (10138/29568)\n",
            "231 391 Loss: 1.753 | Acc: 34.308% (10188/29696)\n",
            "232 391 Loss: 1.753 | Acc: 34.301% (10230/29824)\n",
            "233 391 Loss: 1.753 | Acc: 34.342% (10286/29952)\n",
            "234 391 Loss: 1.753 | Acc: 34.355% (10334/30080)\n",
            "235 391 Loss: 1.752 | Acc: 34.375% (10384/30208)\n",
            "236 391 Loss: 1.752 | Acc: 34.382% (10430/30336)\n",
            "237 391 Loss: 1.752 | Acc: 34.372% (10471/30464)\n",
            "238 391 Loss: 1.753 | Acc: 34.365% (10513/30592)\n",
            "239 391 Loss: 1.752 | Acc: 34.391% (10565/30720)\n",
            "240 391 Loss: 1.751 | Acc: 34.407% (10614/30848)\n",
            "241 391 Loss: 1.751 | Acc: 34.404% (10657/30976)\n",
            "242 391 Loss: 1.751 | Acc: 34.426% (10708/31104)\n",
            "243 391 Loss: 1.751 | Acc: 34.426% (10752/31232)\n",
            "244 391 Loss: 1.750 | Acc: 34.423% (10795/31360)\n",
            "245 391 Loss: 1.750 | Acc: 34.426% (10840/31488)\n",
            "246 391 Loss: 1.750 | Acc: 34.422% (10883/31616)\n",
            "247 391 Loss: 1.749 | Acc: 34.441% (10933/31744)\n",
            "248 391 Loss: 1.749 | Acc: 34.453% (10981/31872)\n",
            "249 391 Loss: 1.748 | Acc: 34.456% (11026/32000)\n",
            "250 391 Loss: 1.748 | Acc: 34.465% (11073/32128)\n",
            "251 391 Loss: 1.747 | Acc: 34.511% (11132/32256)\n",
            "252 391 Loss: 1.747 | Acc: 34.495% (11171/32384)\n",
            "253 391 Loss: 1.747 | Acc: 34.501% (11217/32512)\n",
            "254 391 Loss: 1.747 | Acc: 34.494% (11259/32640)\n",
            "255 391 Loss: 1.747 | Acc: 34.512% (11309/32768)\n",
            "256 391 Loss: 1.747 | Acc: 34.518% (11355/32896)\n",
            "257 391 Loss: 1.747 | Acc: 34.539% (11406/33024)\n",
            "258 391 Loss: 1.746 | Acc: 34.559% (11457/33152)\n",
            "259 391 Loss: 1.747 | Acc: 34.558% (11501/33280)\n",
            "260 391 Loss: 1.747 | Acc: 34.573% (11550/33408)\n",
            "261 391 Loss: 1.746 | Acc: 34.581% (11597/33536)\n",
            "262 391 Loss: 1.746 | Acc: 34.589% (11644/33664)\n",
            "263 391 Loss: 1.746 | Acc: 34.579% (11685/33792)\n",
            "264 391 Loss: 1.746 | Acc: 34.599% (11736/33920)\n",
            "265 391 Loss: 1.746 | Acc: 34.613% (11785/34048)\n",
            "266 391 Loss: 1.746 | Acc: 34.630% (11835/34176)\n",
            "267 391 Loss: 1.745 | Acc: 34.661% (11890/34304)\n",
            "268 391 Loss: 1.745 | Acc: 34.680% (11941/34432)\n",
            "269 391 Loss: 1.745 | Acc: 34.693% (11990/34560)\n",
            "270 391 Loss: 1.745 | Acc: 34.704% (12038/34688)\n",
            "271 391 Loss: 1.744 | Acc: 34.751% (12099/34816)\n",
            "272 391 Loss: 1.744 | Acc: 34.773% (12151/34944)\n",
            "273 391 Loss: 1.744 | Acc: 34.777% (12197/35072)\n",
            "274 391 Loss: 1.743 | Acc: 34.784% (12244/35200)\n",
            "275 391 Loss: 1.743 | Acc: 34.814% (12299/35328)\n",
            "276 391 Loss: 1.742 | Acc: 34.790% (12335/35456)\n",
            "277 391 Loss: 1.742 | Acc: 34.799% (12383/35584)\n",
            "278 391 Loss: 1.742 | Acc: 34.829% (12438/35712)\n",
            "279 391 Loss: 1.742 | Acc: 34.824% (12481/35840)\n",
            "280 391 Loss: 1.741 | Acc: 34.831% (12528/35968)\n",
            "281 391 Loss: 1.741 | Acc: 34.818% (12568/36096)\n",
            "282 391 Loss: 1.741 | Acc: 34.817% (12612/36224)\n",
            "283 391 Loss: 1.741 | Acc: 34.840% (12665/36352)\n",
            "284 391 Loss: 1.741 | Acc: 34.860% (12717/36480)\n",
            "285 391 Loss: 1.741 | Acc: 34.861% (12762/36608)\n",
            "286 391 Loss: 1.740 | Acc: 34.857% (12805/36736)\n",
            "287 391 Loss: 1.740 | Acc: 34.861% (12851/36864)\n",
            "288 391 Loss: 1.740 | Acc: 34.883% (12904/36992)\n",
            "289 391 Loss: 1.740 | Acc: 34.884% (12949/37120)\n",
            "290 391 Loss: 1.740 | Acc: 34.885% (12994/37248)\n",
            "291 391 Loss: 1.740 | Acc: 34.862% (13030/37376)\n",
            "292 391 Loss: 1.740 | Acc: 34.858% (13073/37504)\n",
            "293 391 Loss: 1.740 | Acc: 34.845% (13113/37632)\n",
            "294 391 Loss: 1.740 | Acc: 34.836% (13154/37760)\n",
            "295 391 Loss: 1.739 | Acc: 34.845% (13202/37888)\n",
            "296 391 Loss: 1.739 | Acc: 34.862% (13253/38016)\n",
            "297 391 Loss: 1.739 | Acc: 34.878% (13304/38144)\n",
            "298 391 Loss: 1.738 | Acc: 34.898% (13356/38272)\n",
            "299 391 Loss: 1.738 | Acc: 34.930% (13413/38400)\n",
            "300 391 Loss: 1.737 | Acc: 34.949% (13465/38528)\n",
            "301 391 Loss: 1.737 | Acc: 34.957% (13513/38656)\n",
            "302 391 Loss: 1.737 | Acc: 34.950% (13555/38784)\n",
            "303 391 Loss: 1.737 | Acc: 34.966% (13606/38912)\n",
            "304 391 Loss: 1.737 | Acc: 34.954% (13646/39040)\n",
            "305 391 Loss: 1.737 | Acc: 34.965% (13695/39168)\n",
            "306 391 Loss: 1.736 | Acc: 35.009% (13757/39296)\n",
            "307 391 Loss: 1.736 | Acc: 35.009% (13802/39424)\n",
            "308 391 Loss: 1.736 | Acc: 35.015% (13849/39552)\n",
            "309 391 Loss: 1.736 | Acc: 35.033% (13901/39680)\n",
            "310 391 Loss: 1.736 | Acc: 35.033% (13946/39808)\n",
            "311 391 Loss: 1.735 | Acc: 35.069% (14005/39936)\n",
            "312 391 Loss: 1.736 | Acc: 35.086% (14057/40064)\n",
            "313 391 Loss: 1.735 | Acc: 35.089% (14103/40192)\n",
            "314 391 Loss: 1.735 | Acc: 35.099% (14152/40320)\n",
            "315 391 Loss: 1.735 | Acc: 35.102% (14198/40448)\n",
            "316 391 Loss: 1.734 | Acc: 35.090% (14238/40576)\n",
            "317 391 Loss: 1.734 | Acc: 35.087% (14282/40704)\n",
            "318 391 Loss: 1.734 | Acc: 35.083% (14325/40832)\n",
            "319 391 Loss: 1.733 | Acc: 35.071% (14365/40960)\n",
            "320 391 Loss: 1.733 | Acc: 35.105% (14424/41088)\n",
            "321 391 Loss: 1.733 | Acc: 35.098% (14466/41216)\n",
            "322 391 Loss: 1.733 | Acc: 35.127% (14523/41344)\n",
            "323 391 Loss: 1.732 | Acc: 35.142% (14574/41472)\n",
            "324 391 Loss: 1.732 | Acc: 35.161% (14627/41600)\n",
            "325 391 Loss: 1.732 | Acc: 35.168% (14675/41728)\n",
            "326 391 Loss: 1.732 | Acc: 35.192% (14730/41856)\n",
            "327 391 Loss: 1.732 | Acc: 35.218% (14786/41984)\n",
            "328 391 Loss: 1.732 | Acc: 35.220% (14832/42112)\n",
            "329 391 Loss: 1.731 | Acc: 35.251% (14890/42240)\n",
            "330 391 Loss: 1.731 | Acc: 35.267% (14942/42368)\n",
            "331 391 Loss: 1.730 | Acc: 35.286% (14995/42496)\n",
            "332 391 Loss: 1.730 | Acc: 35.302% (15047/42624)\n",
            "333 391 Loss: 1.730 | Acc: 35.311% (15096/42752)\n",
            "334 391 Loss: 1.729 | Acc: 35.340% (15154/42880)\n",
            "335 391 Loss: 1.729 | Acc: 35.361% (15208/43008)\n",
            "336 391 Loss: 1.729 | Acc: 35.353% (15250/43136)\n",
            "337 391 Loss: 1.728 | Acc: 35.362% (15299/43264)\n",
            "338 391 Loss: 1.728 | Acc: 35.331% (15331/43392)\n",
            "339 391 Loss: 1.728 | Acc: 35.361% (15389/43520)\n",
            "340 391 Loss: 1.727 | Acc: 35.360% (15434/43648)\n",
            "341 391 Loss: 1.727 | Acc: 35.364% (15481/43776)\n",
            "342 391 Loss: 1.727 | Acc: 35.386% (15536/43904)\n",
            "343 391 Loss: 1.726 | Acc: 35.402% (15588/44032)\n",
            "344 391 Loss: 1.726 | Acc: 35.392% (15629/44160)\n",
            "345 391 Loss: 1.726 | Acc: 35.405% (15680/44288)\n",
            "346 391 Loss: 1.726 | Acc: 35.399% (15723/44416)\n",
            "347 391 Loss: 1.726 | Acc: 35.408% (15772/44544)\n",
            "348 391 Loss: 1.726 | Acc: 35.411% (15819/44672)\n",
            "349 391 Loss: 1.726 | Acc: 35.417% (15867/44800)\n",
            "350 391 Loss: 1.726 | Acc: 35.426% (15916/44928)\n",
            "351 391 Loss: 1.725 | Acc: 35.451% (15973/45056)\n",
            "352 391 Loss: 1.725 | Acc: 35.462% (16023/45184)\n",
            "353 391 Loss: 1.725 | Acc: 35.470% (16072/45312)\n",
            "354 391 Loss: 1.725 | Acc: 35.471% (16118/45440)\n",
            "355 391 Loss: 1.724 | Acc: 35.494% (16174/45568)\n",
            "356 391 Loss: 1.724 | Acc: 35.509% (16226/45696)\n",
            "357 391 Loss: 1.724 | Acc: 35.534% (16283/45824)\n",
            "358 391 Loss: 1.723 | Acc: 35.559% (16340/45952)\n",
            "359 391 Loss: 1.723 | Acc: 35.558% (16385/46080)\n",
            "360 391 Loss: 1.723 | Acc: 35.567% (16435/46208)\n",
            "361 391 Loss: 1.723 | Acc: 35.571% (16482/46336)\n",
            "362 391 Loss: 1.723 | Acc: 35.572% (16528/46464)\n",
            "363 391 Loss: 1.722 | Acc: 35.588% (16581/46592)\n",
            "364 391 Loss: 1.722 | Acc: 35.606% (16635/46720)\n",
            "365 391 Loss: 1.722 | Acc: 35.600% (16678/46848)\n",
            "366 391 Loss: 1.722 | Acc: 35.610% (16728/46976)\n",
            "367 391 Loss: 1.721 | Acc: 35.606% (16772/47104)\n",
            "368 391 Loss: 1.721 | Acc: 35.637% (16832/47232)\n",
            "369 391 Loss: 1.720 | Acc: 35.648% (16883/47360)\n",
            "370 391 Loss: 1.720 | Acc: 35.660% (16934/47488)\n",
            "371 391 Loss: 1.720 | Acc: 35.658% (16979/47616)\n",
            "372 391 Loss: 1.720 | Acc: 35.667% (17029/47744)\n",
            "373 391 Loss: 1.720 | Acc: 35.678% (17080/47872)\n",
            "374 391 Loss: 1.720 | Acc: 35.683% (17128/48000)\n",
            "375 391 Loss: 1.720 | Acc: 35.684% (17174/48128)\n",
            "376 391 Loss: 1.720 | Acc: 35.687% (17221/48256)\n",
            "377 391 Loss: 1.720 | Acc: 35.696% (17271/48384)\n",
            "378 391 Loss: 1.720 | Acc: 35.686% (17312/48512)\n",
            "379 391 Loss: 1.720 | Acc: 35.705% (17367/48640)\n",
            "380 391 Loss: 1.720 | Acc: 35.708% (17414/48768)\n",
            "381 391 Loss: 1.720 | Acc: 35.710% (17461/48896)\n",
            "382 391 Loss: 1.720 | Acc: 35.721% (17512/49024)\n",
            "383 391 Loss: 1.720 | Acc: 35.726% (17560/49152)\n",
            "384 391 Loss: 1.720 | Acc: 35.737% (17611/49280)\n",
            "385 391 Loss: 1.719 | Acc: 35.755% (17666/49408)\n",
            "386 391 Loss: 1.720 | Acc: 35.750% (17709/49536)\n",
            "387 391 Loss: 1.719 | Acc: 35.752% (17756/49664)\n",
            "388 391 Loss: 1.719 | Acc: 35.781% (17816/49792)\n",
            "389 391 Loss: 1.719 | Acc: 35.779% (17861/49920)\n",
            "390 391 Loss: 1.719 | Acc: 35.776% (17888/50000)\n",
            "0 100 Loss: 1.608 | Acc: 46.000% (46/100)\n",
            "1 100 Loss: 1.645 | Acc: 42.000% (84/200)\n",
            "2 100 Loss: 1.603 | Acc: 43.667% (131/300)\n",
            "3 100 Loss: 1.630 | Acc: 42.500% (170/400)\n",
            "4 100 Loss: 1.640 | Acc: 41.200% (206/500)\n",
            "5 100 Loss: 1.641 | Acc: 41.667% (250/600)\n",
            "6 100 Loss: 1.644 | Acc: 40.714% (285/700)\n",
            "7 100 Loss: 1.666 | Acc: 40.125% (321/800)\n",
            "8 100 Loss: 1.660 | Acc: 40.556% (365/900)\n",
            "9 100 Loss: 1.639 | Acc: 41.400% (414/1000)\n",
            "10 100 Loss: 1.632 | Acc: 41.727% (459/1100)\n",
            "11 100 Loss: 1.631 | Acc: 41.750% (501/1200)\n",
            "12 100 Loss: 1.642 | Acc: 41.154% (535/1300)\n",
            "13 100 Loss: 1.648 | Acc: 40.643% (569/1400)\n",
            "14 100 Loss: 1.641 | Acc: 41.000% (615/1500)\n",
            "15 100 Loss: 1.644 | Acc: 40.875% (654/1600)\n",
            "16 100 Loss: 1.645 | Acc: 41.353% (703/1700)\n",
            "17 100 Loss: 1.647 | Acc: 41.722% (751/1800)\n",
            "18 100 Loss: 1.643 | Acc: 41.947% (797/1900)\n",
            "19 100 Loss: 1.646 | Acc: 41.550% (831/2000)\n",
            "20 100 Loss: 1.645 | Acc: 41.476% (871/2100)\n",
            "21 100 Loss: 1.647 | Acc: 41.500% (913/2200)\n",
            "22 100 Loss: 1.643 | Acc: 41.391% (952/2300)\n",
            "23 100 Loss: 1.645 | Acc: 41.292% (991/2400)\n",
            "24 100 Loss: 1.644 | Acc: 41.440% (1036/2500)\n",
            "25 100 Loss: 1.655 | Acc: 41.000% (1066/2600)\n",
            "26 100 Loss: 1.653 | Acc: 41.037% (1108/2700)\n",
            "27 100 Loss: 1.652 | Acc: 41.179% (1153/2800)\n",
            "28 100 Loss: 1.654 | Acc: 41.103% (1192/2900)\n",
            "29 100 Loss: 1.648 | Acc: 40.867% (1226/3000)\n",
            "30 100 Loss: 1.643 | Acc: 41.000% (1271/3100)\n",
            "31 100 Loss: 1.641 | Acc: 40.875% (1308/3200)\n",
            "32 100 Loss: 1.642 | Acc: 40.606% (1340/3300)\n",
            "33 100 Loss: 1.641 | Acc: 40.647% (1382/3400)\n",
            "34 100 Loss: 1.643 | Acc: 40.657% (1423/3500)\n",
            "35 100 Loss: 1.639 | Acc: 40.917% (1473/3600)\n",
            "36 100 Loss: 1.638 | Acc: 40.838% (1511/3700)\n",
            "37 100 Loss: 1.634 | Acc: 41.105% (1562/3800)\n",
            "38 100 Loss: 1.636 | Acc: 41.179% (1606/3900)\n",
            "39 100 Loss: 1.636 | Acc: 41.175% (1647/4000)\n",
            "40 100 Loss: 1.640 | Acc: 41.024% (1682/4100)\n",
            "41 100 Loss: 1.642 | Acc: 41.095% (1726/4200)\n",
            "42 100 Loss: 1.643 | Acc: 41.023% (1764/4300)\n",
            "43 100 Loss: 1.640 | Acc: 41.091% (1808/4400)\n",
            "44 100 Loss: 1.644 | Acc: 40.933% (1842/4500)\n",
            "45 100 Loss: 1.641 | Acc: 41.000% (1886/4600)\n",
            "46 100 Loss: 1.640 | Acc: 40.979% (1926/4700)\n",
            "47 100 Loss: 1.640 | Acc: 40.958% (1966/4800)\n",
            "48 100 Loss: 1.636 | Acc: 41.122% (2015/4900)\n",
            "49 100 Loss: 1.634 | Acc: 41.140% (2057/5000)\n",
            "50 100 Loss: 1.635 | Acc: 41.059% (2094/5100)\n",
            "51 100 Loss: 1.634 | Acc: 41.115% (2138/5200)\n",
            "52 100 Loss: 1.634 | Acc: 41.038% (2175/5300)\n",
            "53 100 Loss: 1.636 | Acc: 41.000% (2214/5400)\n",
            "54 100 Loss: 1.639 | Acc: 40.982% (2254/5500)\n",
            "55 100 Loss: 1.641 | Acc: 40.929% (2292/5600)\n",
            "56 100 Loss: 1.646 | Acc: 40.877% (2330/5700)\n",
            "57 100 Loss: 1.642 | Acc: 41.017% (2379/5800)\n",
            "58 100 Loss: 1.643 | Acc: 40.932% (2415/5900)\n",
            "59 100 Loss: 1.642 | Acc: 40.933% (2456/6000)\n",
            "60 100 Loss: 1.643 | Acc: 40.885% (2494/6100)\n",
            "61 100 Loss: 1.644 | Acc: 40.790% (2529/6200)\n",
            "62 100 Loss: 1.646 | Acc: 40.730% (2566/6300)\n",
            "63 100 Loss: 1.645 | Acc: 40.734% (2607/6400)\n",
            "64 100 Loss: 1.645 | Acc: 40.677% (2644/6500)\n",
            "65 100 Loss: 1.645 | Acc: 40.667% (2684/6600)\n",
            "66 100 Loss: 1.645 | Acc: 40.597% (2720/6700)\n",
            "67 100 Loss: 1.644 | Acc: 40.632% (2763/6800)\n",
            "68 100 Loss: 1.643 | Acc: 40.667% (2806/6900)\n",
            "69 100 Loss: 1.646 | Acc: 40.557% (2839/7000)\n",
            "70 100 Loss: 1.647 | Acc: 40.465% (2873/7100)\n",
            "71 100 Loss: 1.646 | Acc: 40.528% (2918/7200)\n",
            "72 100 Loss: 1.644 | Acc: 40.507% (2957/7300)\n",
            "73 100 Loss: 1.643 | Acc: 40.500% (2997/7400)\n",
            "74 100 Loss: 1.643 | Acc: 40.453% (3034/7500)\n",
            "75 100 Loss: 1.642 | Acc: 40.526% (3080/7600)\n",
            "76 100 Loss: 1.640 | Acc: 40.558% (3123/7700)\n",
            "77 100 Loss: 1.641 | Acc: 40.615% (3168/7800)\n",
            "78 100 Loss: 1.640 | Acc: 40.696% (3215/7900)\n",
            "79 100 Loss: 1.640 | Acc: 40.638% (3251/8000)\n",
            "80 100 Loss: 1.638 | Acc: 40.704% (3297/8100)\n",
            "81 100 Loss: 1.638 | Acc: 40.622% (3331/8200)\n",
            "82 100 Loss: 1.638 | Acc: 40.590% (3369/8300)\n",
            "83 100 Loss: 1.638 | Acc: 40.583% (3409/8400)\n",
            "84 100 Loss: 1.639 | Acc: 40.482% (3441/8500)\n",
            "85 100 Loss: 1.638 | Acc: 40.465% (3480/8600)\n",
            "86 100 Loss: 1.639 | Acc: 40.448% (3519/8700)\n",
            "87 100 Loss: 1.638 | Acc: 40.500% (3564/8800)\n",
            "88 100 Loss: 1.638 | Acc: 40.517% (3606/8900)\n",
            "89 100 Loss: 1.638 | Acc: 40.567% (3651/9000)\n",
            "90 100 Loss: 1.637 | Acc: 40.538% (3689/9100)\n",
            "91 100 Loss: 1.638 | Acc: 40.533% (3729/9200)\n",
            "92 100 Loss: 1.636 | Acc: 40.634% (3779/9300)\n",
            "93 100 Loss: 1.637 | Acc: 40.564% (3813/9400)\n",
            "94 100 Loss: 1.636 | Acc: 40.621% (3859/9500)\n",
            "95 100 Loss: 1.634 | Acc: 40.750% (3912/9600)\n",
            "96 100 Loss: 1.633 | Acc: 40.773% (3955/9700)\n",
            "97 100 Loss: 1.634 | Acc: 40.714% (3990/9800)\n",
            "98 100 Loss: 1.634 | Acc: 40.646% (4024/9900)\n",
            "99 100 Loss: 1.634 | Acc: 40.610% (4061/10000)\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 1.663 | Acc: 44.531% (57/128)\n",
            "1 391 Loss: 1.687 | Acc: 39.453% (101/256)\n",
            "2 391 Loss: 1.711 | Acc: 37.240% (143/384)\n",
            "3 391 Loss: 1.722 | Acc: 37.891% (194/512)\n",
            "4 391 Loss: 1.738 | Acc: 37.031% (237/640)\n",
            "5 391 Loss: 1.751 | Acc: 36.458% (280/768)\n",
            "6 391 Loss: 1.752 | Acc: 35.379% (317/896)\n",
            "7 391 Loss: 1.749 | Acc: 35.547% (364/1024)\n",
            "8 391 Loss: 1.749 | Acc: 35.503% (409/1152)\n",
            "9 391 Loss: 1.740 | Acc: 35.703% (457/1280)\n",
            "10 391 Loss: 1.744 | Acc: 36.151% (509/1408)\n",
            "11 391 Loss: 1.738 | Acc: 36.133% (555/1536)\n",
            "12 391 Loss: 1.742 | Acc: 35.757% (595/1664)\n",
            "13 391 Loss: 1.734 | Acc: 36.105% (647/1792)\n",
            "14 391 Loss: 1.732 | Acc: 36.354% (698/1920)\n",
            "15 391 Loss: 1.723 | Acc: 36.816% (754/2048)\n",
            "16 391 Loss: 1.724 | Acc: 36.949% (804/2176)\n",
            "17 391 Loss: 1.723 | Acc: 37.023% (853/2304)\n",
            "18 391 Loss: 1.723 | Acc: 37.212% (905/2432)\n",
            "19 391 Loss: 1.724 | Acc: 37.227% (953/2560)\n",
            "20 391 Loss: 1.719 | Acc: 37.388% (1005/2688)\n",
            "21 391 Loss: 1.713 | Acc: 37.287% (1050/2816)\n",
            "22 391 Loss: 1.712 | Acc: 36.990% (1089/2944)\n",
            "23 391 Loss: 1.709 | Acc: 37.174% (1142/3072)\n",
            "24 391 Loss: 1.704 | Acc: 37.250% (1192/3200)\n",
            "25 391 Loss: 1.699 | Acc: 37.320% (1242/3328)\n",
            "26 391 Loss: 1.697 | Acc: 37.442% (1294/3456)\n",
            "27 391 Loss: 1.694 | Acc: 37.500% (1344/3584)\n",
            "28 391 Loss: 1.688 | Acc: 37.716% (1400/3712)\n",
            "29 391 Loss: 1.688 | Acc: 37.969% (1458/3840)\n",
            "30 391 Loss: 1.692 | Acc: 37.853% (1502/3968)\n",
            "31 391 Loss: 1.690 | Acc: 37.915% (1553/4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-pp89lAXH6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}